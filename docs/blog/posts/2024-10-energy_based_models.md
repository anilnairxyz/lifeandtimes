---
draft: false
date: 2024-10-09
slug: energy_based_models
categories:
  - genai
tags:
  - ai
  - neural networks
  - ebm
  - energy-based models
---

# Energy-Based Models (EBMs)

**Energy-Based Models (EBMs)** offer a highly flexible framework for generative modeling. While VAEs and Flows restrict the model architecture to ensure tractability, EBMs allow us to use *any* function $f_\theta(\mathbf{x})$ to define a probability distribution.

The core idea is borrowed from Boltzmann distributions in physics: we assign a scalar "energy" to every configuration $\mathbf{x}$, where lower energy corresponds to higher probability.

<!-- more -->

$$
p_\theta(\mathbf{x}) = \frac{1}{Z(\theta)} \exp(-E_\theta(\mathbf{x}))
$$

Here, $E_\theta(\mathbf{x})$ is the energy function (parameterized by a neural network), and $Z(\theta)$ is the **partition function** (normalization constant):

$$
Z(\theta) = \int \exp(-E_\theta(\mathbf{x})) d\mathbf{x}
$$

## The Partition Function Problem

The flexibility of EBMs comes at a cost: $Z(\theta)$ is an integral over the entire high-dimensional input space, making it **intractable** to compute or optimize directly. This leads to the "Curse of Dimensionality" where standard Maximum Likelihood Estimation is difficult because we cannot calculate the likelihood.

## Training EBMs

Several techniques have been developed to train EBMs without explicitly calculating $Z(\theta)$.

### Contrastive Divergence (CD)
Used famously in **Restricted Boltzmann Machines (RBMs)**, CD approximates the gradient of the log-likelihood using a short MCMC chain (Markov Chain Monte Carlo). It contrasts the energy of real data points (positive phase) with the energy of "dreamed" samples generated by the model (negative phase).
- **Goal**: Lower the energy of real data, raise the energy of model samples.

### Noise Contrastive Estimation (NCE)
NCE bypasses the partition function by treating density estimation as a binary classification problem.
- Train a discriminator to distinguish between real data (label 1) and noise samples (label 0) drawn from a known noise distribution.
- Under optimal conditions, the learned discriminator density approximates the true data density.

### Score Matching
Another approach is to ignore the density values themselves and model the **score function**â€”the gradient of the log-likelihood with respect to the data:

$$
s_\theta(\mathbf{x}) = \nabla_\mathbf{x} \log p_\theta(\mathbf{x}) = -\nabla_\mathbf{x} E_\theta(\mathbf{x})
$$

Notice that $\nabla_\mathbf{x} \log Z(\theta) = 0$ because $Z(\theta)$ is constant with respect to $\mathbf{x}$. This elegantly removes the partition function from the equation. Training involves minimizing the **Fisher Divergence** between the model score and the data score (which requires some tricks like denoising or sliced score matching to implement efficiently).

Sampling from trained EBMs usually relies on **Langevin Dynamics**, an iterative process using the gradients of the energy function to move random noise towards low-energy (high-probability) regions.
