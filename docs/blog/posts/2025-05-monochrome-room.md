---
title: The Monochrome Room
date: 2025-05-05
slug: monochrome-room
categories:
  - sentience
  - philosophy
tags:
  - qualia
  - physicalism
  - ai
draft: false
---

# The Monochrome Room

Today, a friend and I were discussing the Mary’s Room thought experiment. Despite its age, and the dramatically different world in which it was conceived, the experiment continues to resurface whenever questions of consciousness, understanding and sentience arise - and for good reason. It isolates a central tension between having a complete description of the world and being acquainted with it through experience. That tension turns out to be especially relevant when we examine how modern AI systems are trained and what kind of “knowledge” they can plausibly acquire.

<!-- more -->

The thought experiment, introduced by Frank Jackson, asks us to imagine Mary, a brilliant neuroscientist who has lived her entire life in a black and white environment. From within this constrained setting, Mary learns everything there is to know about the physical processes involved in human colour vision: the physics of light, the biological and neural pathways, colour perception, and their derived meaning. One day, Mary leaves the room and experiences colour for the first time. The question is simple but destabilising: does Mary learn something new?

Jackson’s own answer was yes, and from this he drew a rather strong conclusion: if Mary gains new knowledge despite already knowing all the physical facts, then physicalism is incomplete. There must be facts about conscious experience — he calls them qualia — that are not captured by physical description alone. On this reading, the experiment argues for some form of dualism: even if the world is physically constituted, experience introduces “extra physical qualia” over and above the physical ones.

Many philosophers (including Jackson himself, later on), however, have argued for weaker conclusions. Mary does gain something upon seeing colour, but what she gains is not a new fact about the world. Instead, she acquires a new kind of access to what she already knew: an experiential or practical familiarity rather than additional propositional knowledge. The novelty lies in the mode of knowing, not in the ontology of what is known. Physicalism, on this interpretation, remains intact.

It is this weaker conclusion that I find most plausible. I am not persuaded that Mary’s experience forces us to posit non-physical properties or irreducible qualia. Physical processes are sufficient to explain her new experience. But accepting physicalism does not dissolve the deeper insight of the thought experiment. Mary’s Room still reveals a structural gap between complete description and lived acquaintance — between knowing about an experience and actually experiencing it.

It is precisely this gap, stripped of its metaphysical excesses, that becomes illuminating when we turn to artificial intelligence, especially the LLMs that seem to be the most popular candidates in the public perception to acquire general intelligence capabilities.

### Description Without Presence

Large Language Models are trained through the large-scale compression of descriptions: text produced by humans about the world, about themselves, and about their experiences. From this training, they acquire impressive declarative knowledge. They can answer factual questions, wrestle with abstract concepts, and even recount plausible narratives that resemble episodic memory.

But this knowledge is acquired in a way that is fundamentally unlike how humans or animals learn. Biological cognition is shaped through situated interaction: through perception, action, correction, and consequence. Experience is not merely imbibed; it is undergone. The organism has a stake and often is at stake in the world it encounters and that incentivises what it learns and how it learns it.

LLMs, by contrast, are epistemically saturated yet situationally absent. They do not learn by being wrong in the world. They learn by minimising loss over representations of how others have described being wrong.

This difference is not cosmetic. It is structural.

### Representation Is Not Acquaintance

Mary’s predicament illustrates a gap between complete representation and lived acquaintance. Even if one surmises that this gap is not ontological — that there are no non-physical qualia involved — there definitely does remain an epistemic one. There are forms of knowing that arise only through participation, not through possession of information.

Modern AI systems mirror Mary in an important sense. They may internalise extraordinarily rich models of the world, but those models remain unmoored. They do not arise from the system’s own encounters, nor are they constrained by the system’s own vulnerability to error and the associated fear of harm, or the incentive to succeed on account of the elation of reward.

A model can predict perfectly and still never care whether the prediction is true. It can represent hunger, pain, or danger without ever being exposed to stakes that make those representations matter.

### The Illusion of Progress Through Scale

Recent advances in AI seem all too often driven by the belief that increasing representational power — more parameters, more data, more modalities — will eventually bridge this gap. Vision, audio, and other sensory inputs are added in systems such as humanoid robots and self driving cars, as if sensory bandwidth alone were the missing ingredient.

But this is a category error. Multimodality expands the surface area of representation; it does not introduce subjective significance. Adding more channels of input does not create a point of view. It merely enriches the description available to a system that still lacks a locus within the world described.

What is missing is not information, but presence.

### Hitting the Epistemological Wall

These deliberations lead us to the inevitable conclusion that AI systems trained on current paradigms will invariably hit the epistemological wall.

This is not to claim that artificial systems cannot be sentient. It is to claim that sentience, if it exists, is not an informational achievement. It is not something that emerges automatically from more representation, better models, or broader data.

If sentience arises, it must arise from a system being in the world in a way that matters to it. Where error has consequences, success has stakes, and experience is not merely described but endured.

Modern AI systems, for all their prowess, fail to scale that wall
