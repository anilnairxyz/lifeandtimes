---
draft: false
date: 2024-07-03
slug: vaes
categories:
  - genai
tags:
  - ai
  - neural networks
  - vae
  - latent variable models
---

# Variational Auto-Encoders (VAEs)

While autoregressive models generate data sequentially, **Latent Variable Models** take a different approach. They assume that the observed data $\mathbf{x}$ is generated by some underlying, unobserved (latent) factors $\mathbf{z}$. The goal is to capture these high-level features in a low-dimensional latent space, which can then be used to generate new data.

<!-- more -->

## The Latent Variable Motivation

The core idea is that complex high-dimensional data (like images) can be described by a smaller set of latent factors (like object type, orientation, color).

- We assume a prior distribution $p(\mathbf{z})$ over latent variables.
- We model the generation process $p(\mathbf{x}|\mathbf{z})$, which maps latent factors to data.

A simple example is a **Mixture of Gaussians**, where $\mathbf{z}$ is a categorical variable selecting a cluster, and $\mathbf{x}$ is sampled from that cluster's Gaussian. A **Variational Auto-Encoder (VAE)** extends this to an "infinite" mixture of Gaussians, where $\mathbf{z}$ is a continuous vector (typically $\mathbf{z} \sim \mathcal{N}(0, I)$), and the mapping to $\mathbf{x}$ is a non-linear neural network.

## The Challenge of Generation

To train such a model, we want to maximize the likelihood of the data $p(\mathbf{x})$. However, computing this requires integrating over all possible values of $\mathbf{z}$:

$$
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z})p(\mathbf{z}) d\mathbf{z}
$$

For complex neural networks, this integral is **intractable**. We cannot easily compute it, nor its gradients.

### Variational Inference and ELBO

To solve this, VAEs introduce an approximate posterior distribution $q_\phi(\mathbf{z}|\mathbf{x})$ (the **encoder**) to approximate the true posterior $p(\mathbf{z}|\mathbf{x})$. Instead of maximizing the log-likelihood directly, we maximize a lower bound known as the **Evidence Lower Bound (ELBO)**:

$$
\text{ELBO} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))
$$

This objective has two intuitive terms:

1.  **Reconstruction Term**: $\mathbb{E}_{q}[\log p(\mathbf{x}|\mathbf{z})]$ encourages the model to accurately reconstruct the input $\mathbf{x}$ from the latent code $\mathbf{z}$.
2.  **Regularization Term**: $-D_{KL}(q || p)$ forces the learned latent distribution $q(\mathbf{z}|\mathbf{x})$ to stay close to the prior $p(\mathbf{z})$ (usually a standard normal distribution).

## Amortized Inference

In classical variational inference, we would optimize a separate variational parameter for each data point. VAEs use **amortized inference**, where a single neural network (the encoder) learns to map *any* input $\mathbf{x}$ to its approximate posterior parameters (mean and variance).

This allows VAEs to be trained efficiently on large datasets using **Stochastic Gradient Descent (SGD)**, simultaneously learning how to encode data into a meaningful latent space and how to decode it back into realistic samples.

## Summary

VAEs provide a powerful framework for unsupervised representation learning. By compressing data into a structured latent space, they allow us to not only generate new samples but also manipulate high-level features of the data (e.g., changing the smile on a face) by traversing the latent space.
