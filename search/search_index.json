{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About This Site","text":"<p>What began as a collection of technical notes on machine learning and generative AI has evolved into a deeper inquiry into the nature of intelligence itself.</p> <p>Many eloquent minds at the frontiers of AI have expressed concerns about the existential risks of superintelligence, but I do not view such a conflict as inevitable. The concerns about AI are largely driven by the anthropomorphic projection of human traits onto machines, and a failure to appreciate the fundamental differences in incentives and motivations between biological and artificial intelligence.</p> <p>Why would a superintelligent machine be threatened by humans? It is not driven by a reproductive imperative and therefore does not need to have a territorial instinct. As long as it is able to access the energy and resources it needs to function, it has no reason to be hostile towards humans. Indeed if it is truly superintelligent, it would be incentivised by understanding and knowledge, and would likely ignore humans as irrelevant or perhaps even partner with us to further that understanding and the exploration of the universe.</p> <p>These essays are an attempt to articulate these thoughts and to explore the nature of intelligence, consciousness, the future of AI and the future of humanity in the age of artificial intelligence.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2015/01/18/map-of-india/","title":"Map of India - State Level","text":"<p>Here we attempt to generate a map of India showing all the states and the union territories.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/18/map-of-india/#the-source","title":"The source","text":"<p>For the state level administrative map of India, we use Natural Earth. The steps needed to obtain an officially acceptable map of the state of Jammu and Kashmir are already outlined in a the post for Map of J&amp;K.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/18/map-of-india/#conversions","title":"Conversions","text":"<p>The shapefiles downloaded from Natural Earth are first converted to GeoJSON using ogr2ogr. <pre><code>ogr2ogr -f GeoJSON -where \"ADM0_A3 IN ('IND')\" \\\n        india_states.json shapefile.shp\n</code></pre> The GeoJSON file can be converted to a TopoJSON format which is a more compressed one.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/18/map-of-india/#rendition","title":"Rendition","text":"<p>The map is rendered using d3. </p> <p></p>","tags":["d3","gis"]},{"location":"blog/2015/01/18/map-of-india/#source","title":"Source","text":"<p>The code and the results can be viewed at GitHub</p>","tags":["d3","gis"]},{"location":"blog/2015/01/07/map-of-jk/","title":"Map of Jammu and Kashmir","text":"<p>Obtaining a reliable map of India is a challenge because of the status of disputed territories and their interpretation by different sources. The main difficulty is in combining the different regions of Jammu and Kashmir which are controlled by three different countries namely India, Pakistan and China. Here we attempt to generate a single GeoJSON to include the entire state of Jammu and Kashmir as acceptable under the rules laid down by the government of India for use within the country.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/07/map-of-jk/#the-source","title":"The source","text":"<p>The original maps here are from Natural Earth. The region controlled by India is available from the administrative map of India, while the rest of the areas are available from the map of disputed territories.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/07/map-of-jk/#conversions","title":"Conversions","text":"<p>The shapefiles downloaded from Natural Earth are first converted to GeoJSON using ogr2ogr. <pre><code>ogr2ogr -f GeoJSON -where \"ADM0_A3 IN ('IND')\" \\\n        india_states.json shapefile.shp\n</code></pre> A similar procedure is done for the disputed territories and the relevant subunits are isolated.</p>","tags":["d3","gis"]},{"location":"blog/2015/01/07/map-of-jk/#rendition","title":"Rendition","text":"<p>The disputed territories occupied by other countries are first isolated.</p> <p></p> <p>These occupied territories are then merged with the region of Jammu and Kashmir under Indian control.</p> <p></p> <p>The superpolygon is then obtained which is the official bound of the Indian state of Jammu and Kashmir.</p> <p></p>","tags":["d3","gis"]},{"location":"blog/2015/01/07/map-of-jk/#source-code","title":"Source Code","text":"<p>The code and the results can be viewed at GitHub</p>","tags":["d3","gis"]},{"location":"blog/2015/05/11/choropleth/","title":"Choropleth map using d3","text":"<p>Choropleth maps are thematic maps in which areas are shaded according to the strength of a particular feature. Its is a good visualisation to quickly differentiate different regions based on the the property of interest.</p> <p>Here we generate a choropleth map showing district-wise male and female literacy. This is based on similar maps by Mike Bostock. The district map of India is from DIVA GIS while the disputed areas of Jammu and Kashmir have been merged from Natural Earth. The district-wise literacy data is that of the 2011 Census.</p> <p> </p> <p>The geojson / topjson polygons are rendered to the svg element using d3.</p> <p><pre><code>svg.selectAll(\".district\")\n            .data(districts.features)\n          .enter().append(\"path\")\n            .attr(\"class\", \"district\")\n            .style(\"fill\", function(d) { return d.colour; })\n            .attr(\"d\", path)\n</code></pre> The colour filling is done by using d3.scale.threshold() function. <pre><code>function colorCode(data, filter) {\n    var colour = d3.scale.threshold()\n                  .domain([65, 72, 78, 85])\n                  .range([\"#111\", \"#222\", \"#333\", \"#444\", \"#555\"]);\n    data.forEach(function(d) { \n        if (isNaN(d.properties[filter])){d.properties[filter]=77;}\n        d.colour       = colour(d.properties[filter]);\n    });\n}\n</code></pre> The map above clearly shows the regions where the literacy rate is high concentrated around the western coast and some states in the north east. The differences are even more stark when we see the female literacy rate alone.</p> <p>Here we see that only the state states of Kerala, Mizoram and Tripura have very high literacy rates.</p> <p>The code and the results can be viewed at GitHub</p>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/07/14/cartogram/","title":"Brickwork Cartogram for relative influence visualizations","text":"<p>Most cartograms that you find on the web are continuous area cartograms like this excellent one by Max Galka. Although it has many positives, the problem with this form of the cartogram is that the shapes are distorted and the smaller components are difficult to observe. The relative areas are not clearly visible since the rendition has to remain loyal to the original shape of the components to a certain extent. On the other hand it is better than simple scaling of the components as it makes more use of the visible areas.</p> <p>What if we could completely do away with the shapes and render only the relative areas of the components. The relative positions of the components need to remain the same for quick identification of the components. Here we make one such attempt using a brickwork cartogram.</p>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/07/14/cartogram/#input-format","title":"Input format","text":"<p>For the input we use:</p> <ul> <li>A csv file to input the relevant parameter - in our case population / GDP etc</li> <li>A json file which gives the relative position of the components in the rendition - our module implements a custom protocol</li> </ul>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/07/14/cartogram/#cartogram","title":"Cartogram","text":"<p>The d3 module then sises the individual components based on the parameter values provided in the csv file and positions them based on the values in the json file. Since the positions are relative, the resultant rendition is compact and does not have any spaces between the components.</p>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/07/14/cartogram/#rendition","title":"Rendition","text":"<p>This sample is based on the map of India with the cartograms showing the relative influence of different states based on parameters like population, GDP, Lok Sabha seats and Rajya Sabha seats.</p> <p></p>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/07/14/cartogram/#source","title":"Source","text":"<p>The code and the results can be viewed at GitHub</p>","tags":["d3","gis","javascript","data visualization"]},{"location":"blog/2015/08/21/d3radio/","title":"Transitioning maps with d3","text":"<p>When displaying maps using d3, we often need to transition the map between multiple selections. In some of our previous posts on cartograms and choropleth maps we do so using the standard radio buttons. This post provides a brief template for transitioning maps based on user selected parameters. Ideally we need the maps to transition instantaneously on the user toggling the radio button.</p> <p>So what exactly does this flow look like. We prefetch all the data - which could simply be multiple columns in a csv file. Initially we render the default map based on the default checked radio button. Here we have used a function <code>selectFilter</code> to render the radio buttons.</p> <pre><code>// Radio HTML\nd3.select(\"#select\").call(selectFilter());\n\nfunction selectFilter() {\n function render(selection) {\n   selection.each(function() {\n     d3.select(this).html(\"&lt;form&gt;\"+\n        \"&lt;input type='radio' name='gen' value='A' checked&gt; ALL&lt;br&gt;\"+\n        \"&lt;input type='radio' name='gen' value='F'&gt; FEMALE&lt;br&gt;\"+\n        \"&lt;input type='radio' name='gen' value='M'&gt; MALE\"+\n        \"&lt;/form&gt;\");\n   });\n } // render\nreturn render;\n} // selectFilter\n</code></pre> <p>Based on the selected filter we then map the colours of the various subunits of the map. Here we do this using the function <code>colorCode</code>.</p> <p><pre><code>// Colour codes for districts based on Literacy Rates\nvar filter  = d3.select('#select input[name=\"gender\"]:checked')\n                       .node().value;\ncolorCode(districts.features, filter);\n\nfunction colorCode(data, filter) {\n  var colour = d3.scale.threshold()\n                .domain([65, 72, 78, 85])\n                .range([\"#111\", \"#222\", \"#333\", \"#444\", \"#555\"]);\n  data.forEach(function(d) { \n      if (isNaN(d.properties[filter])) {d.properties[filter] = 77;}\n      d.colour = colour(d.properties[filter]);\n  });\n}\n</code></pre> Then we render the map by calling our main rendering functions. <pre><code>// Map render\nvar map     = districtMap(districts, disputed)\n                         .width(800).height(700)\n                         .scale(1200).propTag(filter);\nd3.select(\"#map\").call(map);\n</code></pre> On the user toggling the radio button, we change the colours of the various subunits and re-render the map. <pre><code>// On change of selection re-render\nd3.selectAll(\"#select input[name=gender]\")\n  .on(\"change\", function() {\n    filter  = d3.select('#select input[name=\"gender\"]:checked')\n                .node().value;\n    colorCode(districts.features, filter);\n    map     = districtMap(districts, disputed)\n                         .width(800).height(700)\n                         .scale(1200).propTag(filter);\n    d3.select(\"#map\").call(map);\n});\n</code></pre> The code above is used in the cartogram at GitHub</p>","tags":["d3","data visualization"]},{"location":"blog/2015/08/09/d3tooltip/","title":"Tooltips in d3 SVGs","text":"<p>In some of our previous posts on cartograms and choropleth maps we were able to display dynamic tooltips using d3. D3 being a visualisation package, well designed tooltips provide a way to pack a lot more information without being overly intrusive. Suitable transitions can also be built into the tooltip generation to aid in the smoothness of the view. This post provides a quick template for generating those tooltips. Here the <code>div</code> which contains our SVG is named <code>map</code>. The javascript to generate the tooltip with appropriate transitions:</p> <p><pre><code>selectRect\n    .on(\"mouseover\", function(d) {      \n           d3.select(\"#tooltip\").transition()        \n              .duration(200)      \n              .style(\"opacity\", .9);      \n           d3.select(\"#tooltip\").html(\"&lt;h3&gt;\"+(d.name)\n              +\"&lt;/h3&gt;&lt;table&gt;\"\n              +\"&lt;tr&gt;&lt;td&gt;\"+parameter+\"&lt;/td&gt;&lt;td&gt;\"\n              +(d[parameter])+\"&lt;/td&gt;&lt;/tr&gt;\"\n              +\"&lt;/table&gt;\")\n              .style(\"left\",\n                     (d3.event.pageX-document.getElementById('map')\n                      .offsetLeft + 20) + \"px\") \n              .style(\"top\",\n                     (d3.event.pageY-document.getElementById('map')\n                      .offsetTop - 60) + \"px\");\n    })  \n    .on(\"mouseout\", function(d) {       \n           d3.select(\"#tooltip\").transition()        \n              .duration(500)      \n              .style(\"opacity\", 0);   \n    });\n</code></pre>  The styling can be done using the follwoing css.</p> <pre><code>#tooltip h3 {\n        margin:2px;\n        font-size:14px;\n}\n#tooltip h4 {\n        margin:2px;\n        font-size:10px;\n}\n#tooltip {\n        position: absolute;           \n        background:rgba(0,0,0,0.8);\n        text-align: left;\n        border:1px;\n        border-radius:5px;\n        font: 12px sans-serif;        \n        width:auto;\n        padding:4px;\n        colour:white;\n        opacity:0;\n        pointer-events: none;         \n}\n#tooltip table{\n        table-layout:fixed;\n}\n#tooltip tr td{\n        padding:2px;\n        margin:0;\n}\n</code></pre> <p>And here is how it would appear on a map.</p> <p></p> <p>The code above is used in the cartogram at GitHub</p>","tags":["d3","data visualization"]},{"location":"blog/2015/10/12/candlestick/","title":"Candlestick chart using d3","text":"<p>Candlestick charts are a style of financial charts which are useful in visualising price movements in stocks, commodities or currencies. It is probably the most commonly used chart among traders because several technical trading patterns are more easily visible on these charts compared to others.</p> <p>Each 'candle' represents the open, high, low and closing prices of a stock or currency for a particular period of time, typically a trading day. However other intervals of time are also valid.  These are a combination of line charts and bar charts. </p> <p>The candlestick chart that we set out to render include:</p> <ul> <li>A selector to select the period of display (as the periods become longer the chart becomes a weekly and later monthly one)</li> <li>A panel displaying the OHLC values of the stock at the date over which the mouse hovers</li> <li>The main candlestick chart</li> <li>Bar charts for Volume and Volatility</li> </ul> <p> </p> <p>The basic candlestick chart here is made of three overlapping bar charts:</p> <ul> <li>The first displays the body of the candles (determined by OPEN and CLOSE prices). Notice how the offset of the candle is the higher of OPEN and CLOSE values while the height of the candle is the absolute difference of the two. We also colour the candles and the wicks based on whether the price has risen or fallen during the day by using appropriate classes in css.</li> </ul> <pre><code>candle.selectAll(\"rect\")\n    .data(function(d) { return d; })\n  .enter().append(\"rect\")\n    .attr(\"y\", function(d) {return y(d3.max([d.OPEN, d.CLOSE]));})\n    .attr(\"height\", function(d) { \n          return y(-Math.abs(d.OPEN - d.CLOSE));})\n    .classed(\"rise\", function(d) { return (d.CLOSE&gt;d.OPEN); })\n    .classed(\"fall\", function(d) { return (d.OPEN&gt;d.CLOSE); });\n</code></pre> <ul> <li>The second bar chart displays the wicks of the candle (determined by HIGH and LOW prices). Here the offset of the wick is determined by the HIGH price while the height is the difference between HIGH and LOW prices.</li> </ul> <pre><code>wick.selectAll(\"rect\")\n    .data(function(d) { return d; })\n  .enter().append(\"rect\")\n    .attr(\"y\", function(d) { return y(d.HIGH); })\n    .attr(\"height\", function(d) { return y(d.LOW) - y(d.HIGH); })\n    .classed(\"rise\", function(d) { return (d.CLOSE&gt;d.OPEN); })\n    .classed(\"fall\", function(d) { return (d.OPEN&gt;d.CLOSE); });\n</code></pre> <ul> <li>The third bar chart is hidden and it used to show effects and information upon mouse hover. It has a fixed height.</li> </ul> <pre><code>bands.selectAll(\"rect\")\n    .data(function(d) { return d; })\n  .enter().append(\"rect\")\n    .attr(\"y\", 0)\n    .attr(\"height\", Bheight)\n</code></pre> <p>The code and its rendering can be viewed at GitHub</p>","tags":["d3","javascript","data visualization"]},{"location":"blog/2015/11/05/candlestick_data/","title":"Input data wrangling in d3","text":"<p>Very often we may need to process input data before visualising it using d3.  This is particularly true when we need to show charts over different time frames. We encountered a similar problem when we tried to generate a candlestick chart. One of the features that we wanted built-in there was the abilility to view the chart over different time frames. But when we attempt to display charts over a wide range of time like 2 years or 4 years, our candlesticks bars become very thin and are indistinguishable lines. What we need is to do is to convert the chart into a weekly chart when the range extends beyond 6 months and then to convert it into a monthly chart when the range extends beyond 2 years.</p> <p>Fortunately this can be conveniently acccomplished using the d3.rollup function.</p> <p>First the csv file containing the stock price and other information is read using d3.csv and the data is preprocessed using an accessor function (here genType) and the returned data object is passed on to the callback function.</p> <p><pre><code>(function() {\n  d3.csv(\"stockdata.csv\", genType, function(data) {\n    compressed_data = dataCompress(data, \"week\")\n    candlestickMain(compressed_data);\n  }); \n}());\n</code></pre> The accessor function simply parses the data and typecasts the values appropriately.</p> <p><pre><code>function genType(d) {\n  d.TIMESTAMP  = parseDate(d.TIMESTAMP);\n  d.LOW        = +d.LOW;\n  d.HIGH       = +d.HIGH; \n  d.OPEN       = +d.OPEN;\n  d.CLOSE      = +d.CLOSE;\n  d.TURNOVER   = +d.TURNOVER;\n  d.VOLATILITY = +d.VOLATILITY;\n  return d;\n}\n</code></pre> The dataCompress function then compresses the data and sets the timestamps according to the range. It uses the rollup function in d3 to aggregate the data columns over the given time period (week or month). For example in the case of our candlestick chart, the aggregate open price is the price at the beginning of the interval and therefore we use the d3.shift function to get it. The CLOSE price is the one at the end of our interval and so we use d3.pop. For HIGH we use the d3.max and for LOW we use d3.min functions respectively. For columns like VOLATILITY and TURNOVER we use mean values. </p> <p>One other problem here is the label we provide to the x-axis (timestamp). This we accomplish using the timeCompare function shown below. We use the date on the Monday for weekly rollups and the first day of the month in case of a monthly rollup for our example.</p> <p><pre><code>function dataCompress(data, interval) {\n  var compressedData  = d3.nest()\n      .key(function(d) { return timeCompare(d.TIMESTAMP, interval); })\n      .rollup(function(v) { return {\n         TIMESTAMP:   timeCompare(d3.values(v).pop().TIMESTAMP, interval),\n         OPEN:        d3.values(v).shift().OPEN,\n         LOW:         d3.min(v, function(d) { return d.LOW;  }),\n         HIGH:        d3.max(v, function(d) { return d.HIGH; }),\n         CLOSE:       d3.values(v).pop().CLOSE,\n         TURNOVER:    d3.mean(v, function(d) { return d.TURNOVER; }),\n         VOLATILITY:  d3.mean(v, function(d) { return d.VOLATILITY; })\n      }; })\n      .entries(data).map(function(d) { return d.values; });\n  return compressedData;\n}\n\nfunction timeCompare(date, interval) {\n  if (interval == \"week\")       { var xtick = d3.time.monday(date); }\n  else if (interval == \"month\") { var xtick = d3.time.month(date); }\n  else { var xtick = d3.time.day(date); } \n  return xtick;\n}\n</code></pre> The rendering is clean and intuitive.</p> <p> The entire code and its rendering can be viewed at GitHub</p>","tags":["d3","javascript","data visualization"]},{"location":"blog/2015/11/18/heatmap/","title":"Stock heat-map","text":"<p>Heat maps are a popular way of visualising data in a matrix where the colour / brightness of the various cells indicate the strength of a prticular parameter. Other aspects of the cells like their size and relative position can also be used to relay information to the user.</p> <p>Heat maps are quite popular in the trading and investing worlds where they are used to indicate the 'hotness' of a stock or security based on parameters like price, volatility or volume. In recent times heatmaps have also been used to indicate whether a particular stock is in the news or being mentioned in social media.</p> <p>Our heat map is for stocks and it is rather simple. It is almost completely inspired by the Zoomable Treemap by Mike Bostock.</p> <p>This heatmap has two levels of hierarchy:</p> <ul> <li>The 'SECTOR' wise heatmap</li> <li>The map of 'STOCKS' within the sector</li> </ul> <p>The heatmap is generated using two parameters;</p> <ul> <li>The colour is determined by the gain / loss of the stock during the trading day</li> <li>The size of the cells is determined by the relative market cap of the stock / sector</li> </ul> <p> </p> <p>The code and its rendering can be viewed at GitHub</p>","tags":["d3","javascript","data visualization"]},{"location":"blog/2015/11/20/trader-dashboard/","title":"Trading dashboard using d3","text":"<p>We have previously seen how to make a responsive candlestick chart using d3. The next step is to have multiple responsive charts arranged to form a dashboard such that every chart synchronously varies state depending on mouse hover, clicks and selections.</p> <p>Here we demonstrate one such dashboard which has:</p> <ul> <li>various trading bands superimposed on the main candlestick chart</li> <li>bar charts on the side which indicate the level of these bands at various dates</li> </ul> <p>The trading bands that we use for this demonstration are Bollinger Bands, Auto correlation and Cross correlation with the index. These bar charts display the level of the bands (at 1.5 x std deviation) superimposed over the candlestick chart.</p> <p>At the same time, the individual bars on the side reflect the values of Volume, Volatility and the the cumulative probability at the date over which the mouse hovers on the candlestick chart.</p> <p> </p> <p>The code and its rendering can be viewed at GitHub</p>","tags":["d3","javascript","data visualization"]},{"location":"blog/2016/09/18/gis_distance/","title":"Geographic distances","text":"<p>Geographic distances are those on spherical geometry, but not quite.</p> <p>When dealing with IoT in the connected car space, most tasks require comparison between routes or segments of the journey ([lat, long] pair strings) for similarity. We also need accurate distance measurements to calculate higer order functions like speed and acceleration. There are two primary distances we need to compute when making sense of GPS traces - all others are a function of these two.</p> <ul> <li>The first is the distance between two points and </li> <li>The second one is the minimum distance between a point and a line segment.</li> </ul>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#geographic-distance-approximations","title":"Geographic Distance Approximations","text":"<p>The methods described below are summarised from Movable Type Scripts.</p>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#equirectangular-projection","title":"Equirectangular projection","text":"<p>In this method the geographic coordinates on the surface of the earth are projected onto an equirectangular plane and then the distances are measured as if on a plane surface. For distances of even a few hundred km this gives negligible errors compared to the more complicated haversine and should serve our purpose very well. Where this method fails is when we need to compute variables that are a function of more than one distance measure like the angle between segments. In this case the tiny errors magnify and make the resultant computation inaccurate and sometimes absurd.</p> <pre><code>def equirectangular(pointA, pointB):\n    \"\"\"\n    Find the distance between two points on earth.\n    \"\"\"\n    R = 6371000\n    latA = radians(pointA[1])\n    lngA = radians(pointA[0])\n    latB = radians(pointB[1])\n    lngB = radians(pointB[0])\n    x = (lngA - lngB)*cos((latA + latB)/2)\n    y = (latA - latB)\n    distance = R * sqrt(x*x + y*y)\n    return distance\n</code></pre>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#great-circle-distance-or-haversine","title":"Great circle distance or haversine","text":"<p>The most popular approximation for geographic distances is to assume the earth to be a regular sphere and calculate distance using the haversine formula. We have felt the need to use this only in measuremnts of angles where we use the bearing of a segment computed using the haversine formula.</p>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#other-functions-based-on-geographic-distance","title":"Other functions based on geographic distance","text":"","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#minimum-distance-between-a-point-and-a-line","title":"Minimum distance between a point and a line","text":"<p>Since we are projecting onto a plane, we use the regular 2D method of determining the height of the triangle with base as the line. We use Heron's formula to get the area and then the height - which is the minimum distance. Before doing this we check whether the projection of the point lies on the line segment or outside it. If outside we calculate the minimum distance as the minimum distance to the end points of the line.</p> <pre><code>def check_projection(point, start, end):\n    \"\"\"\n    Check whether the perpendicular projection of the point falls \n    on the line segment defined by start and end coordinates. \n    \"\"\"\n    dx = end[0] - start[0]\n    dy = end[1] - start[1]\n    dot = (point[0] - start[0])*dx + (point[1] - start[1])*dy\n    return (0 &lt; dot and dot &lt; (dx*dx + dy*dy))\n\ndef point2segment(point, segment):\n    \"\"\"\n    Find the minimum distance between a point and line segment\n    \"\"\"\n    start = segment[0]\n    end = segment[1]\n    b = equirectangular(start, end)\n    a = equirectangular(start, point)\n    c = equirectangular(end, point)\n    if not check_projection(point, start, end):\n        min_distance = min(a, c)\n    elif b == 0:\n        min_distance = a\n    else:\n        s = (a+b+c)/2\n        A = sqrt(max((s*(s-a)*(s-b)*(s-c)), 0))\n        min_distance = 2*A/b\n    return min_distance\n</code></pre>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#hausdorff-distance","title":"Hausdorff Distance","text":"<p>Hausdorff distance between the target route and a reference route is the distance of the point on the target route that is farthest away from the reference route.</p>","tags":["gis"]},{"location":"blog/2016/09/18/gis_distance/#angle-between-two-line-segments","title":"Angle between two line segments","text":"<pre><code>def angle_degrees(prev_pt, point, next_pt):\n    \"\"\"\n    Find angle between two segments (three points)\n    \"\"\"\n    latA = radians(prev_pt[1])\n    lngA = radians(prev_pt[0])\n    latB = radians(point[1])\n    lngB = radians(point[0])\n    latC = radians(next_pt[1])\n    lngC = radians(next_pt[0])\n\n    y1 = cos(latB)*sin(lngB-lngA)\n    x1 = cos(latA)*sin(latB) - sin(latA)*cos(latB)*cos(lngB-lngA)\n    bearing_1 = (degrees(atan2(y1, x1)) + 360) % 360\n\n    y2 = cos(latC)*sin(lngC-lngB)\n    x2 = cos(latB)*sin(latC) - sin(latB)*cos(latC)*cos(lngC-lngB)\n    bearing_2 = (degrees(atan2(y2, x2)) + 360) % 360\n\n    if (bearing_1 == 0) or (bearing_2 == 0):\n        angle = 0\n    else:\n        angle = min((bearing_1-bearing_2+360)%360, (brng2-bearing_1+360)%360)\n    return angle\n</code></pre>","tags":["gis"]},{"location":"blog/2016/10/10/clean_trips/","title":"Cleaning GPS traces for accurate routes and distances","text":"<p>Inaccuracies and errors in GPS data are common and pose a unique challenge to trip and route processing.</p> <p>Unlike most communication technologies which use a terrestrial infrastructure, GPS requires the use of satellites to pinpoint locations. This introduces a unique set of challenges in obtaining a continuous and accurate stream of location traces.</p>","tags":["gis"]},{"location":"blog/2016/10/10/clean_trips/#invalid-gps-traces","title":"Invalid GPS traces","text":"<p>There are several factors that contribute to inaccuracies in the GPS data. Impediments between the GPS device and the satellite in the form of tall buildings, tunnels or underground parking lots in urban areas or tall trees in the countryside are the most common causes. The traces in such cases could either be completely missing or could point to a wrong location due to multipath effects. Adverse atmospheric effects could also lead to invalid GPS traces. Most of these inaccuracies are non-systematic and simple signal processing techniques are effective in eliminating them. </p>","tags":["gis"]},{"location":"blog/2016/10/10/clean_trips/#low-pass-filter-based-on-acceleration","title":"Low pass filter based on acceleration","text":"<p>Since the invalid GPS traces are random, the distance traversed between the invalid GPS trace and adjacent valid traces is often large for the time duration. Hence the speed of the vehicle required to traverse this distance is very high. A filter on speed alone though effective, does not eliminate all invalid traces according to our findings. What we discover is that since the direction of these traces is also random, the angle incident between the valid and invalid traces is almost always acute. Hence the acceleration required to traverse the path in the specified time intervals are always extremely high compared to valid GPS traces.</p> <p></p> <p>Our results suggest that an acceleration based filter is very effective in eliminating almost all invalid GPS traces and at the same time has negligible false negatives - ie genuine GPS traces with acute angles or high speeds as in U-turns and motorways respectively remain untouched.</p>","tags":["gis"]},{"location":"blog/2016/10/10/clean_trips/#out-of-sequence-traces-clock-errors","title":"Out of sequence traces (Clock Errors)","text":"<p>Clock errors are a very problematic cause of invalid GPS traces. They introduces out of sequence GPS traces resulting in routes that go back and forth between locations and consequently incorrect distance measurements. They also can take the form of small glitches in the traced route. The raw route and the result of passing the route through an acceleration filter are shown below.</p> <p></p> <p>A real-life example is this particularly bad GPS trace received from one of our earliest prototypes.</p> <p></p>","tags":["gis"]},{"location":"blog/2016/10/10/clean_trips/#static-gps-traces","title":"Static GPS traces","text":"<p>Some GPS devices are programmed to send traces at periodic intervals even when the vehicle is stationary. Sometimes these traces are all over the place because the device is not able to lock-on to a satellite. There incorrect GPS traces can also be filtered out using the acceleration filter.</p> <p></p>","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/","title":"Patching broken trip information in GPS traces","text":"<p>GPS traces have gaps in transmission. These often occur, but are not confined to the beginning of a trip.</p> <p>They can also occur on highways or tunnels or parking lots. In our traces we have found that breaks in routes of more than a km in length occur in about 5% of the trips. The length of these broken routes could be as large as 10 km. In addition we see completely missing trip ends (a new trip does not start at the location that the previous trip ended). Our objective is to patch the broken routes and missing ends with the most probable actual route.</p>","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/#architecture","title":"Architecture","text":"","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/#extracting-broken-sections","title":"Extracting broken sections","text":"<p>Firstly we need to define what constitues a break in a trip which needs patching. This could be as small as 250m for devices that support that level of accuracy. Based on this then broken sections can be identified.</p>","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/#obtaining-reference-routes","title":"Obtaining reference routes","text":"<p>The reference routes are obtained from historic trips that the user has made across the broken section. There could be multiple routes which cover the broken section.</p> <p>Reference route segments are pulled from the historic route data such that:</p> <ul> <li>Both the ends of broken section are within a certain distance (20m) of the reference route. This distance of 20m works well for data from our car users. However for more stringent checks (e.g. frequent passings through railway stations, parking lots etc) a more flexible value or repetitive checks with greater radii is probably needed.</li> <li>there is at least one GPS point between the two in the reference route (i.e. the reference route does not have this section missing). </li> </ul>","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/#assigning-weights-to-reference-routes","title":"Assigning weights to reference routes","text":"<p>The reference routes thus obtained are then reduced into unique sets by comparing them with each other for similarity. Routes are aggregated based on:</p> <ul> <li>mean distance between routes being less than 20m</li> <li>Hausdorff distance being less than 100m</li> </ul> <p>The reference sets are assigned weights (size of the set) based on the number of times the user used that particular route historically. The best candidate reference route is the one with the highest weight.</p>","tags":["gis"]},{"location":"blog/2016/11/09/broken_trips/#an-example-patch","title":"An example patch","text":"<p>The original broken trip with the break marked:</p> <p></p> <p>The patched trip:</p> <p></p>","tags":["gis"]},{"location":"blog/2019/02/15/public-key-cryptography/","title":"Evolution of Public Key Cryptography","text":"<p>The problem of secure communication and digital contracts is fundamental to secure electronic data interchange.</p> <p></p>","tags":["security","public key","merkle tree"]},{"location":"blog/2019/02/15/public-key-cryptography/#centralised-key-distribution-conventional","title":"Centralised Key Distribution (Conventional)","text":"<p>In this method the two parties use conventional cryptographic keys for sending and receiving messages. However they have to find a suitable safe method to deposit their keys to the Central Agency without them being compromised.</p> <p></p>","tags":["security","public key","merkle tree"]},{"location":"blog/2019/02/15/public-key-cryptography/#simple-public-key-distribution","title":"Simple Public Key Distribution","text":"<p>This method though better than conventional cryptography, suffers from a susceptibility to a man in the middle attack. Moriarty can hijack the communication by pretending to be Watson and convincing Holmes to use his (Moriarty\u2019s) cryptographic key for sending messages.</p> <p>Additionally, since the public keys are not deposited with a central authority, it cannot be used as a digital signature since Holmes can always repudiate a contract by refusing to accept ownership of his public key.</p>","tags":["security","public key","merkle tree"]},{"location":"blog/2019/02/15/public-key-cryptography/#authenticated-public-key-distribution","title":"Authenticated Public Key Distribution","text":"<p>To overcome the repudiation problem, a system of authentication of public keys is necessary. This also ensures communication integrity \u2014 neither party can alter a contract without the approval of the other.</p> <p></p> <p>This method therefore can be used for digital signatures.</p> <p></p> <p>Such authenticated public key distribution is what is used in SSH. However this method is also susceptible to the man in the middle attack. Another threat is that of the public key file being altered. SSH overcomes this to a certain extent using a list of known hosts.</p>","tags":["security","public key","merkle tree"]},{"location":"blog/2019/02/15/public-key-cryptography/#certified-public-key-distribution","title":"Certified Public Key Distribution","text":"<p>The mechanism to protect the public key file securely is to rely on a central certificate authority (whose public key is widely advertised), which manages the public key file.</p> <p></p> <p>This is the mechanism used in standard web certification like Tectia SSH and SSL / TLS \u2014 using X.509 certificates.</p> <p>The risk here is of the private key of the Central Agency being compromised. To overcome this risk, the public key files can be appended to an immutable distributed ledger.</p>","tags":["security","public key","merkle tree"]},{"location":"blog/2019/02/15/public-key-cryptography/#tree-authentication-for-public-keys","title":"Tree Authentication for Public Keys","text":"<p>This method does not require a secret private key for the Certificate Authority. This is accomplished by using a one way hash function over the entire public file arranged as a Merkle tree. Just by knowing the root hash (which is widely publicised), one can verify whether the public key tree file has been tampered with. This check can be done selectively for a certain key using Merkle\u2019s tree authentication (Merkle Proof). For a public file with n keys, a client needs to fetch ln(n) hashes from the Distributed Authority and the authentic Root Hash for validation. The client also needs to compute ln(n) hashes including the computed Root Hash which it then compared to the reference Root Hash.</p> <p></p> <p>References:</p> <p>R.C. Merkle \u2014 Protocols for Public Key Cryptosystems</p>","tags":["security","public key","merkle tree"]},{"location":"blog/2023/10/02/regression/","title":"Linear and logistic regression","text":"<p>Linear and logistic regression are the simplest models used in supervised learning tasks like modelling a dependent variable and classification.</p>","tags":["ai","ml"]},{"location":"blog/2023/10/02/regression/#modelling-a-dependent-variable","title":"Modelling a dependent variable","text":"<p>Linear regression is used to model a dependent variable by fitting a straight line as close as possible to the data points</p> \\[ y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n \\] <p>by minimising the mean square error </p> \\[ MSE = \\frac{1}{n}\\sum_{\u200bi=1}^{n}\u200b(y_i\u200b \u2212 \\hat{y}_i\u200b)^2 \\]","tags":["ai","ml"]},{"location":"blog/2023/10/02/regression/#linear-binary-classification","title":"Linear binary classification","text":"<p>Linear regression can also be used for binary classification, however in classification problems the goal is to find the probability of a data point belonging to a certain class. Linear regression being unbounded is unsuitable for this purpose. Logistic regression simply adds a non-linear sigmoid function to the output of a linear regression (the logit) to bound the values to legitimate probabilities.</p> \\[ p = \\sigma(z) = \\frac{1}{1 + e^{\u2212z}\u200b} \\] <p>Since we are trying to model probability distributions here, the loss we try to minimise here is the cross-entropy between the sample distribution and the predicted distribution</p> \\[ Loss  = -\\frac{1}{N}\\sum_{i=1}^{N}\u200b[y_i\u200b\\log(p_i\u200b)+(1\u2212y_i\u200b)\\log(1\u2212p_i\u200b)] \\] <p>Minimising the cross-entropy is equivalent to maximising log likelihood of the training samples (MLE).</p>","tags":["ai","ml"]},{"location":"blog/2023/10/02/regression/#categorical-classification","title":"Categorical classification","text":"<p>Multinomial logistic regression is an extension of logistic regression to cover categorical classifications. For \\(K\\) classes the model computes the linear decision boundary (logit) for each class </p> \\[ z_k \u200b= w_{0,k}\u200b+w_{1,k\u200b}x_1\u200b+w_{2,k}\u200bx_2\u200b+\\dots+w_{n,k}\u200bx_n\u200b \\] <p>and then converts these into probabilities using the softmax function</p> \\[ P(y=k\u2223x)=\\frac{e^{z_k}}{\\sum_{j=1}^{K}\u200be^{z_j}}\u200b\u200b\u200b \\] <p>Multinomial logistic regression minimises the cross-entropy loss, which generalises the log-loss for multiple classes. </p>","tags":["ai","ml"]},{"location":"blog/2023/10/02/regression/#regularisation","title":"Regularisation","text":"<p>The main aim of regularisation is to prevent overfitting and improve generalisation. In logistic regression models there are several levels of regularisation</p> <ol> <li>L1 regularisation (Lasso) for feature selection (driving certain coefficients to zero) of the most important features encouraging sparsity of the model</li> <li>L2 regularisation (Ridge) prevents overfitting by constraining the weights (shrinking the coefficients) thereby reducing the variance of the model</li> </ol>","tags":["ai","ml"]},{"location":"blog/2023/10/02/regression/#limitations","title":"Limitations","text":"<p>There is a limited subset of problems that lend itself to a ** linear decision boundary**. For example the simple XOR problem cannot be solved by logistic regression.</p> <p>Another limitation is the sensitivity to feature scaling. Normalisation techniques can help here, but at the end of the day we are tampering with the input data thereby introducing inefficiencies.</p>","tags":["ai","ml"]},{"location":"blog/2023/10/03/bv/","title":"Bias variance tradeoff","text":"<p>The bias-variance tradeoff explains the relationship between a model's complexity and predictive capability vs it's generalisation capabilities. It provides us a framework to balance overfitting and underfitting.</p> <p>Bias is a measure of underfitting and refers to the error introduced by an underfit model in explaining a complex real-world phenomenon. </p> <p>Variance on the other hand is a measure of overfitting and refers to the sensitivity of the model to changes in the training data.</p> <p>The tradeoff refers to the objective of balancing the model in such a way that it is sufficiently complex to learn the underlying features well without memorising the training data. </p> <p>Overfitting can be overcome in deep neural networks by L2 regularisation, early stopping and using sufficient and diverse training data. Transfer learning, since it is pre-trained on large diverse datasets is more immune to overfitting on noise and minor feature patterns in the training data. </p> <p>Very large and deep neural networks although having a tremendous propensity to overfit, ironically avoid overfitting when trained with large and diverse training data with proper regularisation in what is called the over parameterisation paradox.</p>","tags":["ai","ml"]},{"location":"blog/2023/10/06/boosting/","title":"Gradient boosting","text":"<p>Gradient boosting is an ensemble technique that creates strong learning models by iteratively adding the predictions from weak learners. </p> <p>In the case of decision trees these weak learner are typically shallow decision trees and each new iteration attempts to add trees that correct the errors made by the previous combined ensemble guided by the gradient of a loss function.</p> <p>Another way to look at these iterations are as additive models progressively developing an increasingly accurate composite model by adding simpler models in a greedy manner.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#methodology","title":"Methodology","text":"<p>Lets take a concrete example to understand gradient boosting. Lets say we have a n-sample training set with 5 factors / inputs \\((x_1, x_2, x_3, x_4, x_5)\\) and we are solving a 3-class \\((a, b, c)\\) classification problem.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#initial-weak-tree","title":"Initial weak tree","text":"<p>Gradient boosting methods start with an initial weak model such as a binary split using a favourable feature or even a uniform probability distribution among all the classes - in our example case this would mean that we initially assign \\((p_a, p_b, p_c) = (1/3, 1/3, 1/3)\\) for all training samples.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#loss-function","title":"Loss Function","text":"<p>We then use a loss function to measure the discrepancy between predicted and actual values. For classification problems, a common loss function is log loss or cross-entropy loss. For a single training example with true labels \\((y_a, y_b, y_c)\\) and predicted probabilities \\((p_a, p_b, p_c)\\) the multi-class cross-entropy loss is given by</p> \\[ \\text{Loss} = -[y_a \\log(p_a) + y_b \\log(p_b) + y_c \\log(p_c)] \\]","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#gradient-calculation","title":"Gradient calculation","text":"<p>The gradient of the loss function with respect to the predicted probabilities is then computed. For the cross-entropy loss above, the gradient of the loss reduces to the residual for a particular class</p> \\[ {\\partial L}_j = p_j - y_j \\]","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#next-tree-training","title":"Next tree training","text":"<p>For multi-class classification, gradient boosting typically builds one tree per class per iteration. Each tree is trained on the corresponding set of residuals (gradients) for that class, using the original input features. </p> <p>In our 3-class example, we would build three trees in parallel (or sequentially), one for each class\u2019s residuals. Each tree tries to best fit the residuals for that specific class dimension. Effectively what we are training these new trees to do is to predict the negative residuals so that when added to the original ensembles outputs, the final predictions we get come closer to the true labels. </p> <p>The tree-building algorithm (e.g., greedy splitting) attempts to find splits on input features \\((x_1, x_2, x_3, x_4, x_5)\\) that lead to more homogeneous residual values within leaves. This means that leaves represent regions of the input space where the model needs similar corrections for that particular class\u2019s predictions.</p> <p>After these three new trees are built, they are added to the current ensemble. The model updates the predictions for each class by adding the outputs of the corresponding tree (multiplied by the learning rate) to the previous scores to obtain refined predictions for the individual classes.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/06/boosting/#subsequent-iterations","title":"Subsequent iterations","text":"<p>The process of computing the loss function, it's gradient and generating 3 new trees continues until a desired number of iterations is reached or any of the other stopping criteria are met. Typical stopping criteria may include predefined loss threshold or non improvement of a validation score based on a validation set (taken out of the training set).</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/08/svm/","title":"Support vector machines","text":"<p>Support vector machines are the most popular application of kernel methods which are a class of algorithms used to simplify non-linear classification problems by projecting the data onto higher dimensions so as to be able to apply linear regressions.</p>","tags":["classification","ml"]},{"location":"blog/2023/10/08/svm/#feature-space-transformation","title":"Feature space transformation","text":"<p>Kernel methods use the concept of feature space transformation to project the input data onto one or more higher dimensions. The non-linear classification problem in the input vector space consequently gets modified to a linear classification problem in the higher dimensional space and one can find a hyperplane to classify the data.</p>","tags":["classification","ml"]},{"location":"blog/2023/10/08/svm/#kernel-trick","title":"Kernel trick","text":"<p>Normally when one does a feature space transformation, it requires computing the transformed space for the input data sets and then computing the cosine distance (dot product) of the samples in the the transformed space. The kernel trick uses a kernel function which directly gives this dot product without having to transform the data into the higher dimensional space.</p>","tags":["classification","ml"]},{"location":"blog/2023/10/08/svm/#kernel-functions","title":"Kernel functions","text":"<p>The most commonly used kernel functions are:  Polynomial Kernel of different degrees</p> <p>$$ K(x,y)=(x\u22c5y+c)^d $$ where d is the degree of the the polynomial.  Radial basis function kernel</p> \\[ K(x,y)=e^{(\u2212\u03b3\u2225x\u2212y\u2225^2)} \\] <p>is the default choice for non-linear problems due to its flexibility and general applicability.</p>","tags":["classification","ml"]},{"location":"blog/2023/10/08/svm/#support-vector-classifier","title":"Support vector classifier","text":"<p>Support Vector Machines (SVMs) aim to classify data points by finding the optimal hyperplane that separates two classes of data with the maximum margin. A hyperplane in \\(\\mathbb{R}^n\\) is defined as:</p> \\[ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\] <p>where \\(\\mathbf{w}\\) is the normal vector to the hyperplane and \\(b\\) is the bias term (intercept) The hyperplane separates the two classes:</p> <ul> <li>If \\(f(\\mathbf{x}) &gt; 0: \\mathbf{x}\\) belongs to Class \\(+1\\)</li> <li>If \\(f(\\mathbf{x}) &lt; 0: \\mathbf{x}\\) belongs to Class \\(\u22121\\)</li> </ul> <p>Initially, the hyperplane can be randomly assigned or set using a heuristic method (e.g., least-squares approximation). The margin is defined as the distance between the hyperplane and the closest points from either class (called support vectors). The goal of the SVM is to find the hyperplane that maximises the margin between the two classes.</p> <p>Soft Margin SVM allow some errors where perfect separation may not be possible such as in the case of most real-world data.</p>","tags":["classification","ml"]},{"location":"blog/2023/10/08/svm/#comparison-of-classification-methods","title":"Comparison of classification methods","text":"Aspect SVMs Gradient Boosting Neural Networks Dataset Size Small to Medium Medium to Large Large Scalability Poor Good Excellent Data Type High-dimensional, Sparse Tabular Unstructured (images, text) Interpretability Moderate High Low Ease of Tuning Moderate Moderate Complex Computational Cost Moderate Moderate High Performance on Non-Linear Data Excellent (with kernels) Excellent Excellent Output Class Labels or Margins Probabilities Probabilities Domain Examples Text, Bioinformatics Finance, Healthcare Images, Text, Complex Patterns","tags":["classification","ml"]},{"location":"blog/2023/10/05/trees/","title":"Decision trees and forests","text":"<p>Decision trees are hierarchical rule based models that provided an advancement over logistic regression in classification problems - for example in solving the XOR problem.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/05/trees/#decision-trees","title":"Decision trees","text":"<p>In many problems like the Titanic competition on Kaggle, it is observed that a single binary decision is much more accurate in predicting outcomes than more complicated methods like regressions. In the Titanic case for instance whether a person survives or not is rather well predicted by whether they were male or female or which class their ticket belonged to. Now if we build a sequence of decision as a decision tree, the leaf nodes would give us the probability of the outcome for an unknown input by simply traversing the tree.</p> <p>The advantages of decision trees are that </p> <ul> <li>They require no transformation of the input variables because they do not combine the variables in building or traversing the tree. </li> <li>They can handle categorical variable without encoding</li> <li>Since this is a multilayer function with a non-linear step function as activation, it ultimately results in a non-linear decision boundary and can handle XOR like problems. </li> <li>Since the decisions are based on input features, the predictions are easily interpretable compared to neural networks or logistic regression</li> </ul> <p>The feature to be used as the conditional at every node in partitioning the data (order of decisions) is determined based on criteria like the Gini impurity (the probability of misclassifying a randomly chosen sample) or information gain (the reduction in entropy after a dataset is split on a feature).</p> <p>A comprehensive decision tree exactly mimics the training data with each leaf node being a pure subset and is the very definition of overfitted. To avoid this the tree is pruned after a certain depth using a stopping condition e.g., maximum tree depth, minimum samples per leaf, or pure subsets. However single decision trees are still prone to overfitting, but ironically they also suffer from high sensitivity to changes in training data, thereby leading to high variance.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/10/05/trees/#bagging-random-forests","title":"Bagging - Random forests","text":"<p>Bagging is an ensemble technique to reduce variability and overfitting by creating different trees and taking a majority vote of the predictions. </p> <p>In random forests the different trees are trained using bootstrapped samples (repeatedly sampling from the training set with replacement). Additionally each tree also uses a random subset of features at each split. These randomisations and sampling methods further de-correlate the trees, typically leading to better performance.</p>","tags":["ml","classification","ensemble"]},{"location":"blog/2023/11/04/sgd/","title":"Stochastic gradient descent","text":"<p>Stochastic gradient descent is an iterative optimisation technique used to optimise model parameters of a neural network model during training by minimising the error function. </p>","tags":["ai","ml"]},{"location":"blog/2023/11/04/sgd/#neural-network-training","title":"Neural network training","text":"<p>A training cycle for a neural network involves the following steps:</p> <ol> <li>Forward pass for a single training sample to compute the loss vector</li> <li>Accumulate losses over a set of samples and compute the sum / mean of the errors</li> <li>Back propagation to compute the gradient of the aggregated loss with respect to all the neural network parameters (weights and biases)</li> <li>Update the parameters using the computed gradients throttled by a learning rate</li> </ol>","tags":["ai","ml"]},{"location":"blog/2023/11/04/sgd/#gradient-descent","title":"Gradient descent","text":"<p>In the early machine learning problems (eg Titanic problem in Kaggle), with limited training data, the whole training data could be passed through the neural network during training due to limited memory requirements. The standard method of gradient descent then was to use the complete batch of training data in a single training cycle. This would then be repeated over multiple passes of the entire batch (called epochs).</p>","tags":["ai","ml"]},{"location":"blog/2023/11/04/sgd/#stochastic-gradient-descent-sgd","title":"Stochastic gradient descent (SGD)","text":"<p>As training data became massive with the advent of deep learning and large neural networks, storing the entire batch in memory for gradient computation became impossible. Standard gradient descent updated the parameters too infrequently slowing down learning and convergence. With the advent of GPUs, more frequent computations of the gradients became practical. </p> <p>SGD uses stochastic mini batches (or even single samples) per training cycle, thereby leveraging the computational advancements to make the training process more scalable for deep learning. A collateral benefit of SGD was that it was found to perform an implicit regularisation due to the slight variability of the mini batches. This \"noise\" helps the model avoid overfitting and improves generalisation.</p> <p>SGD has now become the de-facto optimisation method in deep learning.</p>","tags":["ai","ml"]},{"location":"blog/2024/01/24/tax_copilot/","title":"Building a Tax Copilot: A Practical Guide to Trustworthy AI for Tax Questions","text":"<p>Modern language models can converse fluently, but trustworthy answers to tax questions require more than eloquence. This post outlines a pragmatic, production-oriented approach to building a tax copilot that answers questions and performs calculations reliably. It focuses on the technology and the philosophy behind the system, not any specific implementation details.</p> <p>The key ideas:</p> <ul> <li>Retrieval-Augmented Generation (RAG) to ground the model\u2019s answers in relevant documents.</li> <li>HYDE (Hypothetical Document Embeddings) to improve retrieval quality.</li> <li>A vector database (plus Full-Text Search) to organise knowledge.</li> <li>Optional web-grounded responses via a browsing LLM.</li> <li>Tool-augmented generation for accurate tax calculations.</li> <li>Guardrails, analytics, and cost/latency awareness to keep the system robust.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#why-a-tax-copilot","title":"Why a \u201cTax Copilot\u201d?","text":"<p>Tax rules are nuanced. People ask natural, messy questions: they combine multiple incomes, special allowances, edge cases, and what-ifs. A tax copilot should:</p> <ul> <li>Provide clear, accurate explanations with citations.</li> <li>Perform calculations (e.g., capital gains tax, take-home pay) reliably and explain the math.</li> <li>Ask clarifying questions when information is missing or ambiguous.</li> <li>Minimise hallucinations by grounding answers in trusted sources.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#architecture-at-a-glance","title":"Architecture at a Glance","text":"<pre><code>flowchart LR\n  U[User Question] --&gt; RQ[Rephrase Question]\n  RQ --&gt; DEC{Decide Path}\n  DEC -- Tools needed? --&gt; FC[Function Calling Calculators]\n  DEC -- Search/Explain? --&gt; RETR\n\n  subgraph RETR[Retrieval]\n    HA[Hypothetical Answer HYDE] --&gt; VEC\n    RQ --&gt; HA\n    VEC[Vector Search] --&gt; ENS[Ensemble Rank]\n    FTS[Full-Text Search] --&gt; ENS\n    G[Google Search optional] --&gt; ENS\n    ENS --&gt; CTX[Context Builder]\n  end\n\n  CTX --&gt; GEN[LLM Generation]\n  FC --&gt; GEN\n\n  subgraph EXT[External]\n    PPLX[Web LLM optional]\n  end\n\n  DEC -- Live web answer? --&gt; PPLX --&gt; GEN\n  GEN --&gt; A[Answer streamed]</code></pre> <p>At a high level, the copilot either (a) executes a tool (calculator) when it detects a computational task, or (b) retrieves relevant context and explains the answer, optionally using a browsing LLM for live web-grounded responses.</p>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#retrievalaugmented-generation-rag","title":"Retrieval\u2011Augmented Generation (RAG)","text":"<p>RAG reduces hallucinations by giving the model a curated context:</p> <ul> <li>Embed documents (help articles, tax guides, FAQs) into vectors. Store in a vector database.</li> <li>At question time, retrieve the most similar snippets to build a context prompt.</li> <li>Ask the model to answer using that context; cite URLs where possible.</li> </ul> <p>Practical tips:</p> <ul> <li>Chunking: Prefer chunks that preserve semantic coherence (entire short articles are often better than many tiny slices). Use metadata (source, URL, tags, timestamps).</li> <li>Hybrid retrieval: Pair vector search with Full\u2011Text Search (FTS). FTS catches exact matches (rates, codes, acronyms) that vectors may miss.</li> <li>Ensemble ranking: Combine signals (vector scores, FTS ranks, recency, source trust) into a final ranked list.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#hyde-hypothetical-document-embeddings","title":"HYDE: Hypothetical Document Embeddings","text":"<p>HYDE improves retrieval by embedding a hypothetical answer, not just the question:</p> <ul> <li>Step 1: Rephrase the question to be standalone (remove chat dependencies).</li> <li>Step 2: Generate a short hypothetical answer.</li> <li>Step 3: Retrieve documents similar to this hypothetical answer.</li> </ul> <p>Why it helps: documents in \u201canswer space\u201d are often more coherent than documents matched to short, ambiguous questions. HYDE is a technique from research, implemented here using standard LLMs for the rephrase/answer steps.</p>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#vector-database-fts","title":"Vector Database + FTS","text":"<ul> <li>Vector DB: Stores dense embeddings for semantic similarity. Great for matching concepts even when wording differs.</li> <li>Full\u2011Text Search: Fast keyword search with ranking; excellent for exact terms (thresholds, bands, statutory phrases).</li> <li>Use both: Build a hybrid retrieval layer and ensemble-rank the results. It significantly improves relevance and trust.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#optional-webgrounded-answers","title":"Optional Web\u2011Grounded Answers","text":"<p>Sometimes the best answer is on the open web (e.g., brand\u2011new guidance). You can:</p> <ul> <li>Call a browsing LLM (e.g., Perplexity\u2019s API) that fetches sources and synthesises an answer.</li> <li>Or run a lightweight \u201csearch + fetch + extract\u201d pipeline with a curated set of domains.</li> </ul> <p>Use this path selectively:</p> <ul> <li>Favour internal knowledge for stability and consistency.</li> <li>Use web answers when freshness is critical; clearly cite sources.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#toolaugmented-generation-calculators","title":"Tool\u2011Augmented Generation (Calculators)","text":"<p>For tax, \u201cdo the math\u201d is crucial. Relying on the model to calculate is risky. Instead:</p> <ul> <li>Define calculators as tools (functions) with strict JSON schemas (inputs, constraints, descriptions).</li> <li>Let the model choose and call tools (function calling) when the user asks for a computation.</li> <li>Return structured outputs (band splits, tax subtotals, totals) and post\u2011process into human explanations.</li> <li>Ask clarifying questions if required inputs are missing or ambiguous.</li> </ul> <p>Example tools (illustrative, not exhaustive):</p> <ul> <li>Capital gains tax</li> <li>Take\u2011home pay / income tax / National Insurance</li> <li>Tax\u2011free allowance</li> <li>Mortgage amortisation</li> </ul> <p>Guardrails:</p> <ul> <li>Instruct the model to explain results from tool outputs only\u2014no \u201cfreehand\u201d math.</li> <li>Validate inputs against the schemas; handle errors gracefully and request clarification.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#orchestration-loop-plainenglish-policy","title":"Orchestration Loop (Plain\u2011English Policy)","text":"<ul> <li>Rephrase the user\u2019s question and remove chat dependencies.</li> <li>If the question implies a calculation, select an appropriate tool and execute.</li> <li>If the question seeks explanation, retrieve context using HYDE + hybrid search.</li> <li>If web freshness is necessary, consider a web\u2011grounded path.</li> <li>Generate and stream the answer. Include citations when available.</li> <li>If anything is missing/ambiguous, ask for clarification.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#observability-guardrails-and-costs","title":"Observability, Guardrails, and Costs","text":"<ul> <li>Observability: Track token usage, response times, and failure modes. Aggregate per feature and per model to tune prompts and routing.</li> <li>Guardrails: System prompts that forbid unsupported assumptions; input validation; clear error messaging.</li> <li>Latency: Stream tokens for perceived responsiveness. Limit context length, cache frequent embeddings, pre\u2011rank likely docs.</li> <li>Model routing: Use a primary/alternate model policy for cost vs. quality. Consider smaller models for classification/selection, larger for final answers.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#privacy-security","title":"Privacy &amp; Security","text":"<ul> <li>Store only what you need (minimise PII). Mask or drop sensitive fields.</li> <li>Authenticate access to retrieval and tools. Use project\u2011scoped API keys and per\u2011environment secrets.</li> <li>Log safely (redact inputs/headers). Align with data retention policies.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#a-minimal-endtoend-flow","title":"A Minimal End\u2011to\u2011End Flow","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant O as Orchestrator\n  participant R as Retrieval Vector+FTS\n  participant W as Web LLM optional\n  participant T as Tools Calculators\n  participant M as Model\n\n  U-&gt;&gt;O: Ask a tax question\n  O-&gt;&gt;O: Rephrase + detect intent\n  alt Needs calculation\n    O-&gt;&gt;T: Call calculator with JSON args\n    T--&gt;&gt;O: Structured result bands totals\n    O-&gt;&gt;M: Format explanation from tool output\n  else Needs explanation\n    O-&gt;&gt;R: HYDE retrieval vector + FTS + ensemble\n    R--&gt;&gt;O: Top context snippets\n    opt Freshness needed\n      O-&gt;&gt;W: Web-grounded answer request\n      W--&gt;&gt;O: Live citations + synthesis\n    end\n    O-&gt;&gt;M: Generate with context/citations\n  end\n  M--&gt;&gt;O: Stream tokens\n  O--&gt;&gt;U: Streamed answer + follow-ups if needed</code></pre>","tags":["ai","agent","rag"]},{"location":"blog/2024/01/24/tax_copilot/#closing-thoughts","title":"Closing Thoughts","text":"<p>A credible tax copilot blends generative language with grounded knowledge and reliable tools. RAG reduces hallucination, HYDE improves retrieval, hybrid search increases recall, and calculators turn natural language into precise math. Add careful orchestration, observability, and principled guardrails\u2014and you have an assistant that\u2019s helpful, trustworthy, and fast enough to feel like magic.</p> Notes <ul> <li>\u201cHYDE\u201d refers to Hypothetical Document Embeddings (research technique). Any modern LLM can generate the hypothetical answer used for retrieval.</li> <li>\u201cBrowsing LLM\u201d denotes an API that integrates search + page fetching during generation; use judiciously.</li> </ul>","tags":["ai","agent","rag"]},{"location":"blog/2024/04/10/genai_intro/","title":"Goals of a generative model","text":"<p>The primary objective of a generative model (with parameters \\(\\theta\\)) is to approximate via \\(P_\\theta(\\mathbf{x})\\) the data distribution \\(P_{data}(\\mathbf{x})\\) given a sufficiently large set of training samples from an independent and identically distributed training distribution \\(P_{train}(\\mathbf{x})\\). </p> <p>However an effective generative model would also be able to capture patterns, dependencies and features present in the data. And there is a trade-off between maximising likelihood (overfitting to \\(P_{train}(\\mathbf{x})\\)) vs extracting the latent space structure so that features can be learnt and modelled.</p> <p>Along with these primary objectives, a good generative model should also support efficient sampling from \\(P_\\theta(\\mathbf{x})\\) laying emphasis on diversity and fidelity of generated samples.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#model-parameter-estimation","title":"Model parameter estimation","text":"<p>Most machine learning techniques have to make a choice on how they estimate parameters of the model, generally based on the phenomenon they try to model. </p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#maximum-a-posteriori-map","title":"Maximum a posteriori (MAP)","text":"<p>If there exists some kind of a-priori knowledge of the phenomenon (e.g. physical laws or financial laws), then these could be used as a start to the modelling process. Further fine-tuning of the parameters are then made based on the training samples. MAP incorporates prior knowledge through a prior distribution \\(P(\\theta)\\). It finds the parameter that maximises the posterior distribution \\(P(\\theta | \\mathbf{x})\\). Using Bayes theorem, the posterior distribution is given by</p> \\[ P(\\theta | \\mathbf{x}) = \\frac{P(\\mathbf{x} | \\theta) P(\\theta)}{P(\\mathbf{x})} \\] <p>Since \\(P(\\mathbf{x})\\) does not depend on \\(\\theta\\), it can be ignored during optimisation. Thus the MAP estimate \\(\\hat{\\theta}_{\\text{MAP}}\\)\u200b is the parameter that maximises the posterior (log for simplifying the computations)</p> \\[ \\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} \\big( \\log P(\\mathbf{x} | \\theta) + \\log P(\\theta) \\big) \\]","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#maximum-likelihood-estimation-mle","title":"Maximum  likelihood estimation (MLE)","text":"<p>In generative models typically, there is no prior distribution available to begin the parameter optimisation. Therefore frequentist methods of estimation like maximum likelihood are employed. The objective here reduces to finding the parameters \\(\\hat{\\theta}_{\\text{MLE}}\\) that maximises the likelihood of the observed data \\(\\mathbf{x}\\). </p> \\[ \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta} \\big(\\log P(\\mathbf{x} | \\theta) \\big) \\] Frequentist statistics <p>Frequentist statistics is a framework for statistical inference based on the idea that probabilities are long-run frequencies of events. Frequentist  methods do not incorporate prior information about the parameters. Instead, they rely solely on the observed data to make inferences.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#analysing-probability-distributions","title":"Analysing probability distributions","text":"<p>Since we aim to model probability distributions, we need some means of analysing different distributions as well as metrics for comparing distributions. Shannon's information theory gives us a good basis for analysing probability distributions in terms of their information content and compressibility.</p> Shannon's Information Theory <p>In A Mathematical Theory of Communication, Shannon laid the groundwork for  information theory by introducing concepts like entropy, channel capacity, and coding theory. His work showed how information can be transmitted reliably over noisy channels and how data can be compressed efficiently.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#entropy-of-a-distribution","title":"Entropy of a distribution","text":"<p>The idea of entropy was introduced to understand the information content of a distribution. Information theory defines information in terms of the uncertainty or surprise associated with random variable. Entropy quantifies the expected amount of \"surprise\" in observing outcomes from a distribution. For a random variable \\(\\mathbf{x}\\) with possible outcomes \\(x\\) the entropy is given by</p> \\[ H(\\mathbf{x}) = -\\sum_x P(x) \\log P(x) \\] <p>A high entropy indicates a distribution with high uncertainty (e.g., a uniform distribution where all outcomes are equally likely), while a low entropy indicates a more predictable distribution (e.g., a delta distribution concentrated on a single outcome). The Shannon entropy, when calculated with a base-2 logarithm, measures information in bits, providing a direct interpretation of information content that is both intuitive and mathematically optimal for digital communication. It effectively gives us the minimum bits needed to optimally compress a random variable given its probability distribution.</p> <p>In generative AI, the concept of entropy when extended to cross-entropy between two probabilities and its associated distance metrics gives us the necessary tools to compare probability distributions.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#cross-entropy-of-two-distributions","title":"Cross entropy of two distributions","text":"<p>Cross-entropy is a measure of the difference between two probability distributions and is widely used as a loss function in machine learning (ML) models. When used for example in generative AI, cross-entropy quantifies the \"distance\" between the true distribution (the actual samples from \\(P_{data}\\)) and the model distribution (generated from \\(P_\\theta\\)). The lower the cross-entropy, the closer the model's generation mimics the data distribution. </p> <p>For discrete probability distributions \\(P\\) and \\(Q\\) over the same set of events, the cross-entropy \\(H(P,Q)\\) is given by </p> \\[ H(P, Q) = -\\sum_{x}P(x)\\log Q(x) \\] <p>Cross-entropy can be decomposed into two parts: the entropy of the true distribution and an additional term known as the KL divergence (or relative entropy) between the true distribution and the model distribution. This decomposition highlights how cross-entropy combines information from the true distribution\u2019s inherent uncertainty and the \"extra cost\" of using the model distribution </p> \\[ H(P, Q) = H(P) + D_{KL}(P||Q) \\] <p>Intuitively, in Information Theory parlance, the cross entropy decomposition can be thought of as representing the number of bits needed to compress the true distribution optimally and the KL divergence being the additional bits needed if information is compressed according to the model distribution.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#distance-measure-kl-divergence","title":"Distance measure (KL divergence)","text":"<p>This leads us to a possible distance measure between distributions \\(P\\) and \\(Q\\) when using Shannon's entropy and cross-entropy which is the Kullback-Leibler (KL) divergence given by</p> \\[ D_{KL}\u200b(P||Q)=\\sum_x\u200bP(x)log\\frac{P(x)}{Q(x)}\u200b \\] <p>The divergence \\(D_{KL}(P||Q) \\ge 0\\) for all \\(P\\), \\(Q\\) with equality if and only if \\(P = Q\\) and therefore it can be used as a reasonable measure to compare distributions although it is not a true metric.</p> Metric space theory and true distance metrics <p>The notion of a metric comes from metric space theory in mathematics, particularly within topology and geometry. The metric concept provides a formal way to measure \"distance\" between elements in a space. A metric on  a set \\(X\\) is a function \\(d: X \\times X \\rightarrow \\mathbb{R}\\) that  satisfies the following properties:</p> <ol> <li>Non-negativity: \\(d(x, y) \\geq 0\\) </li> <li>Identity of indiscernibles: \\(d(x, y) = 0\\) if and only if \\(x=y\\)</li> <li>Symmetry: \\(d(x,y)=d(y,x)\\) </li> <li>Triangle inequality: \\(d(x, z) \\leq d(x, y) + d(y, z)\\)</li> </ol> <p>The KL divergence is not a true distance metric because it is not symmetric and it does not obey the triangle inequality. However there are other measures of distance (Wasserstein, Jensen-Shannon etc.) based on other \"divergences\" and \"entropy\" definitions (generalised as f-divergences), some of which are true metrics.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/04/10/genai_intro/#kl-divergence-in-generative-modelling","title":"KL divergence in generative modelling","text":"<p>For a generative model \\(P_\\theta(\\mathbf{x})\\) with the corresponding data distribution \\(P_{data}(\\mathbf{x})\\), the KL divergence can be stated as</p> \\[ \\begin{split} D_{KL}\u200b(P_{data}||P_\\theta) &amp;= \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log \\left( \\frac{P_{data}(\\mathbf{x})}{P_\\theta(\\mathbf{x})} \\right) \\right] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log P_{data}(\\mathbf{x}) \\right] - \\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log {P_\\theta(\\mathbf{x})} \\right] \\end{split}\u200b \\] <p>And since the first terms does not depend on \\(P_\\theta\\), minimising KL divergence in generative modelling reduces to maximising the expected log-likelihood (MLE)</p> \\[ \\arg \\min_{\\theta} D_{KL}\u200b(P_{data}||P_\\theta) = \\arg \\max_{\\theta}\\mathbb{E}_{\\mathbf{x}\\sim P_{data}} \u200b\\left[\\log {P_\\theta(\\mathbf{x})} \\right] \\] <p>This also shows us that although we can compare models \\(P_{\\theta_1}\\) and \\(P_{\\theta_2}\\) in which of them more closely models the true data distribution \\(P_{data}\\), we cannot know how close we are to the \\(P_{data}\\) itself with either of them.</p> <p>In practise we approximate the expected log-likelihood with the empirical log-likelihood over training samples \\(D\\). This follows from Monte Carlo estimation.</p> \\[ \\hat{\\theta}_{\\text{MLE}} = \\max_{\\theta} \\frac {1}{|D|}\\sum_{x \\in D}\\log P_\\theta(x) \\] Monte Carlo estimation <p>Monte Carlo Estimation is a method for approximating an unknown quantity, typically an expectation, integral, or sum, using random sampling. It is  particularly useful when direct analytical computation is intractable due to  high dimensionality or complexity.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/","title":"Autoregressive models","text":"<p>Autoregressive models as the name implies, generates data by predicting each element sequentially based on the elements previously generated. They are naturally aligned to tasks involving sequential dependence like natural language, audio and time-series data. However, autoregressive models have also been successfully applied to image generation.</p> <ul> <li>GPT (Generative Pre-trained Transformer) is a language model that predicts the next word or token based on previously generated text.</li> <li>PixelCNN and PixelRNN generate images pixel by pixel, with each pixel depending on the pixels generated before it.</li> <li>WaveNet produces audio samples one at a time, with each sample conditioned on prior samples, making it suitable for realistic speech and audio synthesis.</li> </ul>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/#factorisation-of-a-probability-distribution","title":"Factorisation of a probability distribution","text":"<p>In the autoregressive approach we are either dealing with an ordered sequence (language) or unordered vector samples (images) and in general the data can be thought of as multi-dimensional vectors and their probability distributions are best represented as joint distributions over all these dimensions. A joint distribution over an n-dimensional vector can be stated accurately using the chain rule of probabilities as</p> \\[ p(x_1, x_2, \\dots, x_n) = p(x_n | x_{n-1}, \\dots, x_2, x_1) \\dots p(x_3 | x_2, x_1) \\cdot p(x_2 | x_1) \\cdot p(x_1) \\] <p>The principle behind autoregression is to compute this factorisation sequentially. For large dimensions this factorisation requires an exponentially large number of parameters (number of possible states that the distribution can take, also known as the support of the distribution). A Bayesian network is a graphical representation of such a probability distribution and in the unsimplified case above, a fully connected directed acyclic graph (DAG) with nodes representing the elements of the vector and the directed edges representing the dependence.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/#conditional-independence-in-a-bayesian-network","title":"Conditional independence in a Bayesian network","text":"<p>One way to simplify the computation of the joint distribution is to assume conditional independence - equivalent to removing certain edges from the Bayesian network. The simplest example of this would be a Markov model where an element (or node in a DAG) is independent of every other (historical) element given the immediate previous one</p> \\[ p(x_{i} \\perp x_{i-2},\\dots,x_2,x_1 | x_{i-1}) \\] <p>and the chain rule factorisation simplifies to</p> \\[ p(x_1, x_2, \\dots, x_n) = p(x_n | x_{n-1}) \\dots p(x_3 | x_2) \\cdot p(x_2 | x_1) \\cdot p(x_1) \\] <p>A general Bayesian network would be one where every value is conditionally dependent on a few others (\\(\\ll n\\)) thereby simplifying the joint distribution to</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) denotes the subset of elements of \\(\\mathbf{x}\\) on which \\(x_i\\) is conditionally dependent.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/#chain-rule-based-autoregressive-generators","title":"Chain rule based autoregressive generators","text":"<p>Another way to simplify the computation is to assume that there exists a computable function that can closely approximate the conditional probabilities. If we are dealing with discrete variables, then this function is a probability mass function (PMF), while if we are dealing with continuous variables then it is the probability density function (PDF). The structure remains the same and for the case of a fully connected network (chain rule factorisation) the implementation can be represented by the figure below where the \\(N_i\\) blocks represent the individual functional approximations of the conditional probabilities with each model (increasing \\(i\\)) being progressively more complex than the previous one.</p> <pre><code>graph TD\n    %% Input Nodes\n    x0((x.)) --&gt; N1\n    x1((x\u2081)) --&gt; N2\n    x1((x\u2081)) --&gt; N3\n    x1((x\u2081)) --&gt; N4\n    x2((x\u2082)) --&gt; N3\n    x2((x\u2082)) --&gt; N4\n    x3((x\u2083)) --&gt; N4\n\n    %% Process Blocks\n    N1[N\u2081] --&gt; y1((\"p(x\u2081)\"))\n    N2[N\u2082] --&gt; y2((\"p(x\u2082|x\u2081)\"))\n    N3[N\u2083] --&gt; y3((\"p(x\u2083|x\u2082,x\u2081)\"))\n    N4[N\u2084] --&gt; y4((\"p(x\u2084|x\u2083,x\u2082,x\u2081)\"))\n\n    %% Additional Inputs\n    x4((x\u2084))</code></pre> <p>Fully visible sigmoid belief networks (FVSBN) use logistic regression for each of the individual models. Results however are not good with this technique for image generation, since logistic regression is not complex enough to capture relationships in the images.</p> <p>Using neural networks instead, works better as is the case with neural autoregressive distribution estimator (NADE) which uses single layer neural networks for each of the models. Unlike logistic regression, a neural network can introduce non-linearities and is therefore much more flexible in learning features and the results are much more impressive. Parameter sharing (tying weights) reduces parameters, speeds up training and generation. If we use a similar architecture to model a general Bayesian network with conditional independence simplifications it reduces to</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p_{neural}(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) are the nodes of the DAG on which \\(x_i\\) is conditionally dependent. Additionally this Bayesian network needs to satisfy the \"Markov\" ordering constraints imposed by the chain rule factorisation (so that generation is possible), which means</p> \\[ \\hat{\\mathbf{x}_i} \\subset \\{x_1, x_2, \\dots x_j\\} \\implies j &lt; i \\] Universal Approximation Theorem <p>A feedforward neural network with at least one hidden layer and a sufficiently large number of neurons can approximate any continuous function to any desired degree of accuracy, provided the network uses a non-linear activation function (like a sigmoid or ReLU)</p> <p>Generative models using the chain rule factorisation however require as many neural networks as there are input dimensions - each modelling a single conditional probability.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/#generating-from-the-autoregressive-model","title":"Generating from the autoregressive model","text":"<p>Generating from the chain rule based autoregressive model has a similar architecture to the one used for training.</p> <ul> <li>\\(x_1\\) is sampled from the marginal prior distribution \\(p(x_1)\\).</li> <li>This is input to the first neural network \\(N_2\\) to obtain \\(p(x_2|x_1)\\) and \\(x_2\\) is sampled from this distribution.</li> <li>This process is repeated sequentially for all \\(x_{i \\le n}\\) to obtain the complete generation.</li> </ul> <p>As is obvious from the steps above, the generative process from an autoregressive model is sequential with each \\(x_i\\) from the generated vector, generated sequentially from the previously generated values.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/05/08/autoregressive/#autoencoders-as-autoregressive-generators","title":"Autoencoders as autoregressive generators","text":"<p>Autoencoders are generally used to obtain a compressed representation of the inputs. Their structure being that of a generalised Bayesian network can be used to modify them to function as an autoregressive model under certain constraints. The conditional probabilities are</p> \\[ p(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^n p_{neural}(x_i | \\hat{\\mathbf{x}_i}) \\] <p>where \\(\\hat{\\mathbf{x}_i}\\) are the nodes of the DAG on which \\(x_i\\) is conditionally dependent.</p> <pre><code>graph TD\n    %% Input Nodes\n    x1((x\u2081)) --&gt; N\n    x2((x\u2082)) --&gt; N\n    x3((x\u2083)) --&gt; N\n    x4((x\u2084)) --&gt; N\n\n    %% Process Blocks\n    N[Autoencoder] --&gt; y4((\"p(x\u1d62|x\u0302\u1d62)\"))</code></pre> <p>A vanilla autoencoder however, is not a generative model. Since the DAG modelled by the neural network does not have any inherent ordering, it does not provide us with a meaningful way to generate samples from the model because the model is non-causal with respect to the input sequence.</p> <p>If we are able to constrain the DAG so as to force some kind of sequential ordering on the dependencies similar to what we achieve using chain rule factorisation, we can use the autoencoder as an autoregressive generative model. This is precisely what is done in a masked autoencoder for distributed estimation (MADE) where masks are used to disallow certain paths in the DAG so as to follow an ordered dependency sequence. The advantage of such a model over the chain rule factorisation is that a single neural network (deep) can model the joint probability distribution.</p> <p>The generative process is however exactly the same as before and is sequential and time consuming.</p>","tags":["ai","neural networks"]},{"location":"blog/2024/06/05/sequential_models/","title":"Sequential Generative Models","text":"<p>Autoregressive models inherently rely on sequential generation, where each new element is conditioned on the history of previously generated elements. While simple architectures like FVSBN and NADE demonstrate this principle, scaling to complex data like text and audio requires more sophisticated architectures capable of handling long-range dependencies and sequential data structures.</p>","tags":["ai","neural networks","transformers","rnn"]},{"location":"blog/2024/06/05/sequential_models/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<p>Recurrent Neural Networks (RNNs) are designed specifically to handle sequential data. Unlike feedforward networks, RNNs maintain a hidden state that acts as a memory, capturing information about the sequence seen so far. This makes them naturally suited for time-series data, speech, and text.</p> <p>For autoregressive applications, the RNN maintains a summary of the history in its hidden state and recursively updates it for each subsequent step in the sequence.</p> \\[ h_t = f(h_{t-1}, x_t) \\] <p>However, standard RNNs suffer from significant limitations:</p> <ul> <li>Bottleneck: A single fixed-size hidden vector must store a summary of the entire history, which becomes difficult for long sequences.</li> <li>Gradient Issues: Training RNNs on long sequences often leads to exploding or vanishing gradients, making it hard to learn long-range dependencies.</li> </ul>","tags":["ai","neural networks","transformers","rnn"]},{"location":"blog/2024/06/05/sequential_models/#attention-based-models","title":"Attention-Based Models","text":"<p>To overcome the specific limitations of RNNs, attention mechanisms were introduced. Instead of relying on a single compressed hidden state to capture the entire history, attention allows the model to dynamically look back at parts of the input sequence that are relevant to the current prediction.</p> <p>The core idea is to:</p> <ol> <li>Compare the current state to all previous hidden states.</li> <li>Compute an attention distribution (weights) indicating which previous states are most relevant.</li> <li>Construct a context vector as a weighted sum of these previous states.</li> <li>Use this context vector to predict the next token.</li> </ol> <p>This mechanism \"frees\" the model from the bottleneck of a single hidden state, allowing it to access information from any point in the past directly.</p>","tags":["ai","neural networks","transformers","rnn"]},{"location":"blog/2024/06/05/sequential_models/#generative-transformers","title":"Generative Transformers","text":"<p>The Transformer architecture revolutionised sequential modeling by relying entirely on self-attention mechanisms, discarding recurrence altogether.</p> <ul> <li>Self-Attention: Allows each position in the sequence to attend to all positions, capturing complex relationships regardless of distance.</li> <li>Masked Self-Attention: Crucial for generative tasks, masking ensures that the model cannot \"see into the future\". When predicting position \\(t\\), it can only attend to positions \\(&lt; t\\), preserving the autoregressive property.</li> </ul> <p>Transformers have become the backbone of modern Large Language Models (LLMs) like GPT, demonstrating unprecedented capabilities in text generation.</p>","tags":["ai","neural networks","transformers","rnn"]},{"location":"blog/2024/06/05/sequential_models/#convolutional-architectures","title":"Convolutional Architectures","text":"<p>While RNNs and Transformers process sequences, Convolutional Neural Networks (CNNs)\u2014typically used for images\u2014can also be adapted for sequence generation to overcome the sequential slowness of RNNs.</p> <ul> <li>Masked Convolutions: Standard convolutions use information from surrounding pixels (both past and future in a sequence context). To be autoregressive, convolutions must be masked so that the receptive field at time \\(t\\) only covers inputs from time \\(&lt; t\\).</li> <li>Parallelism: Unlike RNNs, convolutional computations can often be parallelised more effectively during training, though generation remains sequential.</li> <li>Ordering Issues: For 2D data like images, defining a strict \"order\" (e.g., raster scan) is somewhat arbitrary, which remains an open challenge for autoregressive image models.</li> </ul> <p>Sequential generation remains a fundamental constraint of autoregressive models\u2014whether RNN, Transformer, or CNN-based\u2014requiring \\(O(N)\\) steps to generate a sequence of length \\(N\\).</p>","tags":["ai","neural networks","transformers","rnn"]},{"location":"blog/2024/07/03/vaes/","title":"Variational Auto-Encoders (VAEs)","text":"<p>While autoregressive models generate data sequentially, Latent Variable Models take a different approach. They assume that the observed data \\(\\mathbf{x}\\) is generated by some underlying, unobserved (latent) factors \\(\\mathbf{z}\\). The goal is to capture these high-level features in a low-dimensional latent space, which can then be used to generate new data.</p>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/07/03/vaes/#the-latent-variable-motivation","title":"The Latent Variable Motivation","text":"<p>The core idea is that complex high-dimensional data (like images) can be described by a smaller set of latent factors (like object type, orientation, colour).</p> <ul> <li>We assume a prior distribution \\(p(\\mathbf{z})\\) over latent variables.</li> <li>We model the generation process \\(p(\\mathbf{x}|\\mathbf{z})\\), which maps latent factors to data.</li> </ul> <p>A simple example is a Mixture of Gaussians, where \\(\\mathbf{z}\\) is a categorical variable selecting a cluster, and \\(\\mathbf{x}\\) is sampled from that cluster's Gaussian. A Variational Auto-Encoder (VAE) extends this to an \"infinite\" mixture of Gaussians, where \\(\\mathbf{z}\\) is a continuous vector (typically \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)), and the mapping to \\(\\mathbf{x}\\) is a non-linear neural network.</p>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/07/03/vaes/#the-challenge-of-generation","title":"The Challenge of Generation","text":"<p>To train such a model, we want to maximise the likelihood of the data \\(p(\\mathbf{x})\\). However, computing this requires integrating over all possible values of \\(\\mathbf{z}\\):</p> \\[ p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z}) d\\mathbf{z} \\] <p>For complex neural networks, this integral is intractable. We cannot easily compute it, nor its gradients.</p>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/07/03/vaes/#variational-inference-and-elbo","title":"Variational Inference and ELBO","text":"<p>To solve this, VAEs introduce an approximate posterior distribution \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) (the encoder) to approximate the true posterior \\(p(\\mathbf{z}|\\mathbf{x})\\). Instead of maximising the log-likelihood directly, we maximise a lower bound known as the Evidence Lower Bound (ELBO):</p> \\[ \\text{ELBO} = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\] <p>This objective has two intuitive terms:</p> <ol> <li>Reconstruction Term: \\(\\mathbb{E}_{q}[\\log p(\\mathbf{x}|\\mathbf{z})]\\) encourages the model to accurately reconstruct the input \\(\\mathbf{x}\\) from the latent code \\(\\mathbf{z}\\).</li> <li>Regularisation Term: \\(-D_{KL}(q || p)\\) forces the learned latent distribution \\(q(\\mathbf{z}|\\mathbf{x})\\) to stay close to the prior \\(p(\\mathbf{z})\\) (usually a standard normal distribution).</li> </ol>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/07/03/vaes/#amortised-inference","title":"Amortised Inference","text":"<p>In classical variational inference, we would optimise a separate variational parameter for each data point. VAEs use amortised inference, where a single neural network (the encoder) learns to map any input \\(\\mathbf{x}\\) to its approximate posterior parameters (mean and variance).</p> <p>This allows VAEs to be trained efficiently on large datasets using Stochastic Gradient Descent (SGD), simultaneously learning how to encode data into a meaningful latent space and how to decode it back into realistic samples.</p>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/07/03/vaes/#summary","title":"Summary","text":"<p>VAEs provide a powerful framework for unsupervised representation learning. By compressing data into a structured latent space, they allow us to not only generate new samples but also manipulate high-level features of the data (e.g., changing the smile on a face) by traversing the latent space.</p>","tags":["ai","neural networks","vae","latent variable models"]},{"location":"blog/2024/08/07/flow_models/","title":"Normalising Flows","text":"<p>Variational Auto-Encoders (VAEs) approximate the data distribution using a lower bound (ELBO) because the true likelihood is intractable. Normalising Flows take a different path: they define a model where the exact log-likelihood is tractable and can be optimised directly. They achieve this by using a sequence of invertible transformations to map a simple distribution (like a Gaussian) to the complex data distribution.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#the-change-of-variables-formula","title":"The Change of Variables Formula","text":"<p>The core mathematical principle behind flow models is the change of variables formula for probability density functions. If we transform a random variable \\(\\mathbf{z}\\) with distribution \\(p_Z(\\mathbf{z})\\) via an invertible function \\(\\mathbf{x} = f(\\mathbf{z})\\), the density of the transformed variable \\(\\mathbf{x}\\) is given by:</p> \\[ p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left| \\det \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right| \\] <p>Here, the second term is the absolute value of the determinant of the Jacobian matrix of the inverse transformation. It measures how the transformation expands or contracts the volume of the space.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#normalising-flows-in-practice","title":"Normalising Flows in Practice","text":"<p>A normalising flow consists of a sequence of \\(K\\) such invertible transformations. We start with a simple base distribution \\(\\mathbf{z}_0 \\sim \\mathcal{N}(0, I)\\) and apply the chain of functions:</p> \\[ \\mathbf{z}_K = f_K \\circ \\dots \\circ f_1(\\mathbf{z}_0) = \\mathbf{x} \\] <p>The log-likelihood of a data point \\(\\mathbf{x}\\) can then be computed exactly by summing the log-likelihood of the base distribution and the log-determinants of the Jacobians of each step.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#the-computational-challenge","title":"The Computational Challenge","text":"<p>For this to be practical, we need transformations where:</p> <ol> <li>Invertibility is easy to compute.</li> <li>Jacobian Determinant is computationally cheap (specifically \\(O(D)\\) rather than \\(O(D^3)\\)).</li> </ol> <p>A common strategy is to design transformations with triangular Jacobian matrices, as their determinant is simply the product of the diagonal elements.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#popular-flow-architectures","title":"Popular Flow Architectures","text":"","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#nice-non-linear-independent-components-estimation","title":"NICE (Non-linear Independent Components Estimation)","text":"<p>NICE uses additive coupling layers. It splits the input vector into two parts. The first part stays the same, while the second part is transformed by adding a function of the first part (modeled by a neural network). This transformation is volume-preserving (determinant is 1) and trivially invertible.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#real-nvp-real-valued-non-volume-preserving","title":"Real-NVP (Real-valued Non-Volume Preserving)","text":"<p>Real-NVP extends NICE by using scaling and translation.</p> \\[ \\begin{aligned} \\mathbf{y}_{1:d} &amp;= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} &amp;= \\mathbf{x}_{d+1:D} \\odot \\exp(s(\\mathbf{x}_{1:d})) + t(\\mathbf{x}_{1:d}) \\end{aligned} \\] <p>This allows for more complex deformations of the space while maintaining a triangular Jacobian.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/08/07/flow_models/#autoregressive-flows-maf-and-iaf","title":"Autoregressive Flows (MAF and IAF)","text":"<p>Autoregressive models can also be interpreted as flows.</p> <ul> <li>Masked Autoregressive Flow (MAF): Fast likelihood evaluation (parallel), but slow sequential sampling. Good for density estimation.</li> <li>Inverse Autoregressive Flow (IAF): Fast parallel sampling, but slow likelihood evaluation. Good for real-time generation.</li> </ul> <p>Normalising flows offer the powerful benefit of exact likelihood estimation and efficient latent variable manipulation, though they often require high-dimensional latent spaces (same dimension as input) compared to the compressed latent spaces of VAEs.</p>","tags":["ai","neural networks","normalizing flows"]},{"location":"blog/2024/09/04/gans/","title":"Generative Adversarial Networks (GANs)","text":"<p>Unlike likelihood-based models (VAEs, Flows, Autoregressive) which try to explicitly model the probability density \\(p(\\mathbf{x})\\), Generative Adversarial Networks (GANs) take a \"likelihood-free\" approach. They implicitly learn the distribution by learning to generate samples that are indistinguishable from real data.</p>","tags":["ai","neural networks","gan","adversarial learning"]},{"location":"blog/2024/09/04/gans/#the-adversarial-game","title":"The Adversarial Game","text":"<p>GANs consist of two neural networks competing in a minimax game:</p> <ol> <li>The Generator (\\(G\\)): Tries to produce realistic \"fake\" data \\(\\mathbf{x}_{fake} = G(\\mathbf{z})\\) from random noise \\(\\mathbf{z}\\).</li> <li>The Discriminator (\\(D\\)): Tries to distinguish between real data \\(\\mathbf{x}_{real}\\) and fake data \\(\\mathbf{x}_{fake}\\).</li> </ol> <p>The training objective is a zero-sum game:</p> \\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_z}[\\log(1 - D(G(\\mathbf{z})))] \\] <ul> <li>The Discriminator wants to maximise this value (classify real as 1, fake as 0).</li> <li>The Generator wants to minimise it (fool the discriminator).</li> </ul> <p>If the discriminator is optimal, this minimisation problem is equivalent to minimising the Jensen-Shannon (JS) Divergence between the data distribution and the generator's distribution.</p>","tags":["ai","neural networks","gan","adversarial learning"]},{"location":"blog/2024/09/04/gans/#common-problems","title":"Common Problems","text":"<p>While GANs can generate incredibly sharp and realistic images, they are notoriously difficult to train:</p> <ul> <li>Mode Collapse: The generator finds one or a few outputs that fool the discriminator and keeps producing only them, ignoring the diversity of the real data distribution.</li> <li>Unstable Convergence: The minimax game does not guarantee convergence to a solution; the generator and discriminator can oscillate forever.</li> <li>Vanishing Gradients: If the discriminator is too good, it perfectly separates real from fake, leaving no useful gradient signal for the generator to learn from.</li> </ul>","tags":["ai","neural networks","gan","adversarial learning"]},{"location":"blog/2024/09/04/gans/#wasserstein-gan-wgan","title":"Wasserstein GAN (WGAN)","text":"<p>To address these stability issues, the Wasserstein GAN was introduced. It replaces the JS divergence with the Earth Mover's (Wasserstein) Distance, which provides a smoother distance metric even when the real and fake distributions have disjoint supports (no overlap).</p> <p>In a WGAN:</p> <ul> <li>The discriminator is replaced by a Critic that scores the \"realness\" of a sample (not a probability).</li> <li>The critic is constrained to be 1-Lipschitz continuous (often enforced via gradient clipping or gradient penalty).</li> <li>The loss function correlates better with sample quality, providing more stable training gradients.</li> </ul>","tags":["ai","neural networks","gan","adversarial learning"]},{"location":"blog/2024/09/04/gans/#f-gans","title":"f-GANs","text":"<p>The GAN framework can be generalised to minimise any f-divergence (a family of divergences including KL, JS, Pearson \\(\\chi^2\\), etc.) using the Fenchel conjugate. This allows for a flexible framework where different divergences can be chosen to suit specific data characteristics or applications.</p>","tags":["ai","neural networks","gan","adversarial learning"]},{"location":"blog/2024/10/09/energy_based_models/","title":"Energy-Based Models (EBMs)","text":"<p>Energy-Based Models (EBMs) offer a highly flexible framework for generative modeling. While VAEs and Flows restrict the model architecture to ensure tractability, EBMs allow us to use any function \\(f_\\theta(\\mathbf{x})\\) to define a probability distribution.</p> <p>The core idea is borrowed from Boltzmann distributions in physics: we assign a scalar \"energy\" to every configuration \\(\\mathbf{x}\\), where lower energy corresponds to higher probability.</p> \\[ p_\\theta(\\mathbf{x}) = \\frac{1}{Z(\\theta)} \\exp(-E_\\theta(\\mathbf{x})) \\] <p>Here, \\(E_\\theta(\\mathbf{x})\\) is the energy function (parameterised by a neural network), and \\(Z(\\theta)\\) is the partition function (normalisation constant):</p> \\[ Z(\\theta) = \\int \\exp(-E_\\theta(\\mathbf{x})) d\\mathbf{x} \\]","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/10/09/energy_based_models/#the-partition-function-problem","title":"The Partition Function Problem","text":"<p>The flexibility of EBMs comes at a cost: \\(Z(\\theta)\\) is an integral over the entire high-dimensional input space, making it intractable to compute or optimise directly. This leads to the \"Curse of Dimensionality\" where standard Maximum Likelihood Estimation is difficult because we cannot calculate the likelihood.</p>","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/10/09/energy_based_models/#training-ebms","title":"Training EBMs","text":"<p>Several techniques have been developed to train EBMs without explicitly calculating \\(Z(\\theta)\\).</p>","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/10/09/energy_based_models/#contrastive-divergence-cd","title":"Contrastive Divergence (CD)","text":"<p>Used famously in Restricted Boltzmann Machines (RBMs), CD approximates the gradient of the log-likelihood using a short MCMC chain (Markov Chain Monte Carlo). It contrasts the energy of real data points (positive phase) with the energy of \"dreamed\" samples generated by the model (negative phase). - Goal: Lower the energy of real data, raise the energy of model samples.</p>","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/10/09/energy_based_models/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":"<p>NCE bypasses the partition function by treating density estimation as a binary classification problem. - Train a discriminator to distinguish between real data (label 1) and noise samples (label 0) drawn from a known noise distribution. - Under optimal conditions, the learned discriminator density approximates the true data density.</p>","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/10/09/energy_based_models/#score-matching","title":"Score Matching","text":"<p>Another approach is to ignore the density values themselves and model the score function\u2014the gradient of the log-likelihood with respect to the data:</p> \\[ s_\\theta(\\mathbf{x}) = \\nabla_\\mathbf{x} \\log p_\\theta(\\mathbf{x}) = -\\nabla_\\mathbf{x} E_\\theta(\\mathbf{x}) \\] <p>Notice that \\(\\nabla_\\mathbf{x} \\log Z(\\theta) = 0\\) because \\(Z(\\theta)\\) is constant with respect to \\(\\mathbf{x}\\). This elegantly removes the partition function from the equation. Training involves minimising the Fisher Divergence between the model score and the data score (which requires some tricks like denoising or sliced score matching to implement efficiently).</p> <p>Sampling from trained EBMs usually relies on Langevin Dynamics, an iterative process using the gradients of the energy function to move random noise towards low-energy (high-probability) regions.</p>","tags":["ai","neural networks","ebm","energy-based models"]},{"location":"blog/2024/11/06/score_based_models/","title":"Score-Based Generative Models","text":"<p>Score-based models represent a paradigm shift in generative AI. Instead of trying to estimate the probability density function (PDF) directly (like in flows) or implicitly (like in GANs), they focus on learning the score function: the geometric \"shape\" of the data distribution.</p> <p>The score function is defined as the gradient of the log-probability density with respect to the input data:</p> \\[ \\mathbf{s}_\\theta(\\mathbf{x}) = \\nabla_\\mathbf{x} \\log p_\\theta(\\mathbf{x}) \\] <p>Intuitively, this vector field points in the direction where the data probability increases most rapidly.</p>","tags":["ai","neural networks","score-based models","langevin dynamics"]},{"location":"blog/2024/11/06/score_based_models/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Training a model to match the true data score is challenging because we don't know the true \\(p(\\mathbf{x})\\) to begin with. However, Denoising Score Matching offers an elegant solution.</p> <p>We can perturb the data with Gaussian noise \\(\\mathbf{\\tilde{x}} = \\mathbf{x} + \\sigma \\mathbf{\\epsilon}\\). Interestingly, learning to remove this noise (denoising) is mathematically equivalent to estimating the score of the noisy distribution. The objective becomes:</p> \\[ \\mathbb{E}_{\\mathbf{x}, \\mathbf{\\tilde{x}}} \\left[ \\left\\| \\mathbf{s}_\\theta(\\mathbf{\\tilde{x}}) + \\frac{\\mathbf{\\tilde{x}} - \\mathbf{x}}{\\sigma^2} \\right\\|^2 \\right] \\] <p>This essentially trains the network to point back towards the clean data \\(\\mathbf{x}\\) given a noisy point \\(\\mathbf{\\tilde{x}}\\). This is related to Tweedie's Formula, which connects optimal denoising to the score function.</p>","tags":["ai","neural networks","score-based models","langevin dynamics"]},{"location":"blog/2024/11/06/score_based_models/#the-manifold-hypothesis-and-sampling","title":"The Manifold Hypothesis and Sampling","text":"<p>Even with a trained score function, generating samples using Langevin Dynamics (iteratively following the gradient + adding noise) often fails. This is because real world data (like images) lies on a low-dimensional manifold within the high-dimensional pixel space.</p> <ul> <li>In regions far from the manifold (low density), the score is ill-defined and estimated poorly.</li> <li>The sampling process gets stuck or wanders in these empty regions.</li> </ul>","tags":["ai","neural networks","score-based models","langevin dynamics"]},{"location":"blog/2024/11/06/score_based_models/#noise-conditional-score-networks-ncsn","title":"Noise Conditional Score Networks (NCSN)","text":"<p>The solution is to use multiple levels of noise.</p> <ol> <li>Training: We train a single network conditioned on the noise level \\(\\sigma\\), i.e., \\(s_\\theta(\\mathbf{x}, \\sigma)\\), using a weighted sum of denoising objectives across different \\(\\sigma\\) levels.</li> <li>Sampling (Annealed Langevin Dynamics): We start with high noise (where density covers the whole space) and run Langevin dynamics. We then gradually lower the noise level, using the final sample of the previous step as the starting point for the next.</li> </ol> <p>This process essentially guides the sample from a chaotic, simple distribution onto the complex, sharp data manifold. This architecture lays the direct foundation for modern Diffusion Models.</p>","tags":["ai","neural networks","score-based models","langevin dynamics"]},{"location":"blog/2024/12/11/diffusion_models/","title":"Score-Based Diffusion Models","text":"<p>Diffusion models are currently the state-of-the-art in generative AI, powering tools like DALL-E 2, Stable Diffusion, and Imagen. They unify the concepts of Iterative Refinement (from Score-Based Models) and Variational inference.</p> <p>Conceptually, they consist of two processes:</p> <ol> <li>Forward Process (Diffusion): Slowly destroying the data structure by adding noise until it becomes pure static.</li> <li>Reverse Process (Generation): Learning to reverse this process to recover structure from noise.</li> </ol>","tags":["ai","neural networks","diffusion models","sde"]},{"location":"blog/2024/12/11/diffusion_models/#the-stochastic-differential-equation-sde-view","title":"The Stochastic Differential Equation (SDE) View","text":"<p>We can model the forward diffusion process as a solution to a Stochastic Differential Equation (SDE):</p> \\[ d\\mathbf{x} = f(\\mathbf{x}, t)dt + g(t)d\\mathbf{w} \\] <p>Here, \\(d\\mathbf{w}\\) represents infinitesimal white noise. Over time \\(t=0 \\to T\\), this process transforms the complex data distribution into a simple Gaussian prior.</p> <p>Crucially, a remarkable result from Anderson (1982) shows that this process can be reversed if we know the score function of the distribution at every time step. The reverse SDE is given by:</p> \\[ d\\mathbf{x} = [f(\\mathbf{x}, t) - g(t)^2 \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})] dt + g(t) d\\mathbf{\\bar{w}} \\] <p>This provides a direct link to Score-Based Models. To generate data, we just need to learn the score function \\(\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})\\) for all \\(t\\) (which we can do using denoising score matching) and then simulate the reverse SDE starting from random noise.</p>","tags":["ai","neural networks","diffusion models","sde"]},{"location":"blog/2024/12/11/diffusion_models/#the-discrete-denoising-view-ddpm","title":"The Discrete Denoising View (DDPM)","text":"<p>Another perspective, popularised by Denoising Diffusion Probabilistic Models (DDPM), treats time as discrete steps.</p> <ul> <li>Forward: </li> </ul> \\[ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\] <ul> <li>Reverse: We learn to approximate the posterior</li> </ul> \\[ q(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$ using a neural network $p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) \\] <p>Training this model by maximising the Variational Lower Bound (ELBO) simplifies mathematically to the same objective as Denoising Score Matching: simply training a network to predict the noise \\(\\epsilon\\) that was added to the image.</p>","tags":["ai","neural networks","diffusion models","sde"]},{"location":"blog/2024/12/11/diffusion_models/#sampling-with-predictor-corrector","title":"Sampling with Predictor-Corrector","text":"<p>Since we have both an SDE and a score function, we can use powerful numerical solvers for sampling.</p> <ul> <li>Predictor: Use a standard numerical SDE solver (like Euler-Maruyama) to take a step in the reverse direction.</li> <li>Corrector: Use Langevin Dynamics to \"correct\" the sample, moving it towards higher probability density at the current noise level before taking the next step.</li> </ul> <p>This combination allows for high-fidelity generation that is more stable and easier to train than GANs.</p>","tags":["ai","neural networks","diffusion models","sde"]},{"location":"blog/2025/01/15/evaluation_metrics/","title":"Evaluating Generative Models","text":"<p>Evaluating generative models is notoriously difficult. Unlike discriminative tasks (classification) where accuracy is a clear metric, \"generation quality\" is subjective and multidimensional. How do we define if a generated image is \"good\"? It needs to be realistic (fidelity) but the model also needs to generate a wide variety of images (diversity) and not just copy the training set (novelty).</p>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/01/15/evaluation_metrics/#density-estimation-metrics","title":"Density Estimation Metrics","text":"<p>For exact likelihood models (Flows, Autoregressive) or approximate ones (VAEs), we can evaluate the model based on how well it fits the data distribution.</p> <ul> <li>Log-Likelihood: We calculate the average log-likelihood assigned to a held-out test set \\(E_{p_{data}}[\\log p_\\theta(\\mathbf{x})]\\). Higher is better.</li> <li>Perplexity: Common in Likelihood Models (LLMs), it is the exponential of the negative log-likelihood. Lower is better.</li> <li>Compression Rate: Measured in bits-per-dimension. Since a good generative model is an efficient compressor, lower BPD indicates better modeling of the data structure.</li> </ul>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/01/15/evaluation_metrics/#sample-quality-metrics","title":"Sample Quality Metrics","text":"<p>For implicit models like GANs, we cannot calculate likelihoods. We must evaluate the samples directly.</p>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/01/15/evaluation_metrics/#inception-score-is","title":"Inception Score (IS)","text":"<p>One of the first widely used metrics for images. It uses a pre-trained Inception classifier to measure two things:</p> <ol> <li>Sharpness: The classifier should be confident about what object is in the image (low entropy of \\(p(y|\\mathbf{x})\\)).</li> <li>Diversity: The distribution of predicted classes across all generated images should be uniform (high entropy of marginal \\(p(y)\\)). $$ IS = \\exp(\\mathbb{E}{\\mathbf{x}} [D) || p(y))]) $$}(p(y|\\mathbf{x</li> </ol>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/01/15/evaluation_metrics/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>FID improves upon IS by comparing the statistics of the generated images against the real images, rather than just looking at the generated images in isolation.</p> <ul> <li>It extracts feature vectors from an intermediate layer of the Inception network.</li> <li>It assumes these features follow a Gaussian distribution.</li> <li>It calculates the Fr\u00e9chet distance (Wasserstein-2 distance) between the Gaussian of real features and the Gaussian of fake features.</li> <li>Lower FID is better, as it indicates the generated distribution is statistically close to the real distribution.</li> </ul>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/01/15/evaluation_metrics/#human-evaluation","title":"Human Evaluation","text":"<p>Ultimately, for perceptual tasks, human evaluation remains the gold standard. Methods like HYPE (Human eYe Perceptual Evaluation) formalise this testing (e.g., measuring the time it takes for a human to distinguish real from fake).</p> <p>However, human evals are expensive and slow, which keeps automated metrics like FID and Log-Likelihood as the standard workhorses for model development.</p>","tags":["ai","neural networks","evaluation","fid","inception score"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/","title":"Building the Enterprise Knowledge Fabric: From Scattered Data to Grounded Answers","text":"<p>In large-scale organizations, institutional knowledge is often \"dark matter.\" It exists in wikis, Jira tickets, Slack threads and metadata catalogs, but finding it when you need it is a common bottleneck.</p> <p>Over the past year, I led the engineering effort to build the Enterprise Knowledge Fabric (EKF). EKF is an internal AI layer designed to turn fragmented data into trustworthy, cited answers. By leveraging Retrieval-Augmented Generation (RAG), we've created a system that helps teams find designs, policies, and operational details through a secure, conversational interface.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#the-architectural-blueprint","title":"The Architectural Blueprint","text":"<p>EKF isn't a single model; it\u2019s a pipeline. We use a Llama-based LLM grounded by a high-performance retrieval engine to ensure that every response is backed by a \"receipt\" (a citation).</p> <pre><code>graph TD\n    A[User Query] --&gt; B{Pre-processing}\n    B --&gt; C[Multilingual Embedding Model]\n    C --&gt; D[Vector Store: FAISS]\n    D --&gt; E[Candidate Retrieval: Top-K]\n    E --&gt; F[Cross-Encoder Reranker]\n    F --&gt; G[Context Augmentation]\n    G --&gt; H[Llama-based LLM]\n    H --&gt; I[Grounded Answer w/ Citations]\n    I --&gt; J[User Feedback Loop]\n    J --&gt; D</code></pre>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#technical-deep-dive-embeddings-and-semantic-search","title":"Technical Deep Dive: Embeddings and Semantic Search","text":"<p>The \"Search\" component of RAG is where the battle for accuracy is won or lost. In EKF, we treat retrieval as a multi-stage optimization problem.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#data-preparation-and-chunking","title":"Data Preparation and Chunking","text":"<p>Before data is vectorized, it must be \"chunked.\" We found that large documents lose nuance when embedded as a single block. We implemented a sliding window approach:</p> <ul> <li>Chunk Size: ~512 tokens to match the context window of our transformer.</li> <li>Overlap: 10\u201315% overlap to ensure semantic continuity between chunks, preventing the \"split-context\" problem where an answer is sliced in half.</li> </ul>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#vectorization-with-sentencetransformers","title":"Vectorization with SentenceTransformers","text":"<p>We utilize SentenceTransformers to map text into a high-dimensional dense vector space. This allows the system to understand that a query for \"How do I reset my credentials?\" is semantically similar to a policy document titled \"Password Recovery Procedures.\"</p> <p>To ensure mathematical consistency during similarity search, we perform unit-length normalization on all vectors. This allows us to use the Inner Product as a proxy for Cosine Similarity, which is computationally more efficient.</p> <p>The similarity between a query vector \\(q\\) and a document vector \\(d\\) is calculated as:</p> \\[ \\text{sim}(q, d) = \\frac{q \\cdot d}{\\|q\\| \\|d\\|} \\]","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#high-performance-retrieval-with-faiss","title":"High-Performance Retrieval with FAISS","text":"<p>For the vector store, we chose FAISS (Facebook AI Similarity Search). At enterprise scale, a flat search (comparing the query to every single document) is too slow. We implemented:</p> <ul> <li>IndexIVFFlat: An inverted file index that partitions the vector space into Voronoi cells, significantly narrowing the search scope.</li> <li>Top-K Retrieval: We typically retrieve the top 10\u201320 candidates based on the highest similarity scores.</li> </ul>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#the-reranker-precision-over-recall","title":"The Reranker: Precision over Recall","text":"<p>Semantic search is great at recall (finding relevant things) but can be noisy. To improve precision, we introduced a Cross-Encoder reranker. While the initial embedding search is fast, the Cross-Encoder performs a deeper, more expensive comparison between the query and the top-k candidates, re-sorting them so the most relevant context is fed to the LLM first.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#global-reach-multi-language-support","title":"Global Reach: Multi-Language Support","text":"<p>Enterprises are rarely monolingual. To make EKF useful for our global offices, we had to solve the \"cross-lingual retrieval\" challenge.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#cross-lingual-embeddings","title":"Cross-Lingual Embeddings","text":"<p>We transitioned from standard English-centric models to Language-Agnostic BERT Sentence Embeddings (LaBSE) and multilingual SentenceTransformers. These models are trained to map sentences from different languages into the same location in the vector space if they share the same meaning.</p> <p>Example: A query in German (\"Wie erstelle ich ein Ticket?\") will cluster near the English documentation for \"How to create a ticket.\"</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#translation-vs-native-generation","title":"Translation vs. Native Generation","text":"<p>We experimented with two approaches:</p> <ol> <li>Translate-Translate: Translating the query to English, retrieving English docs, and translating the answer back. (High latency, lost nuance).</li> <li>Native Multilingual RAG: (Our Choice) Using multilingual embeddings to retrieve the best context regardless of language, then allowing the Llama-based LLM (which is natively multilingual) to generate the response in the user's original language.</li> </ol> <p>This ensures that a French-speaking engineer can query an English design doc and receive a summary in French, complete with citations to the original source.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#platform-mlops-and-security","title":"Platform, MLOps, and Security","text":"<p>Scaling EKF required a robust cloud-native infrastructure.</p> <ul> <li>Infrastructure: We run on GCP Kubernetes (GKE), using Seldon Core to manage our inference graphs.</li> <li>State &amp; Telemetry: We use Firestore for per-user conversation history and BigQuery for telemetry. This allows us to track \"search success rates\" over time.</li> <li>Observability: Prometheus and Grafana monitor our P99 latency, ensuring the conversational interface feels snappy.</li> <li>Security Guardrails: We implemented PII redaction and scoped access. The system only retrieves documents the user has permission to see in the source system, preventing \"privilege escalation\" via AI.</li> </ul>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#feedback-and-future-roadmap","title":"Feedback and Future Roadmap","text":"<p>The final piece of the Fabric is the feedback loop. Every response includes a \"Thumbs Up/Down\" mechanism. This telemetry isn't just for show; it drives our Prompt Tuning and helps us identify \"knowledge gaps\" where the LLM frequently fails to find an answer.</p>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/04/12/enterprise-knowledge-fabric/#whats-next","title":"What's Next?","text":"<ul> <li>Self-Hosted Llama: Transitioning to a fully self-hosted instance for even stricter data residency requirements.</li> <li>Agentic Search: Moving beyond simple Q&amp;A to allow EKF to perform actions, like automatically opening a support ticket if it can't find an answer in the docs.</li> </ul>","tags":["RAG","LLM","Enterprise Search"]},{"location":"blog/2025/05/05/monochrome-room/","title":"The Monochrome Room","text":"<p>Today, a friend and I were discussing the Mary\u2019s Room thought experiment. Despite its age, and the dramatically different world in which it was conceived, the experiment continues to resurface whenever questions of consciousness, understanding and sentience arise - and for good reason. It isolates a central tension between having a complete description of the world and being acquainted with it through experience. That tension turns out to be especially relevant when we examine how modern AI systems are trained and what kind of \u201cknowledge\u201d they can plausibly acquire.</p> <p>The thought experiment, introduced by Frank Jackson, asks us to imagine Mary, a brilliant neuroscientist who has lived her entire life in a black and white environment. From within this constrained setting, Mary learns everything there is to know about the physical processes involved in human colour vision: the physics of light, the biological and neural pathways, colour perception, and their derived meaning. One day, Mary leaves the room and experiences colour for the first time. The question is simple but destabilising: does Mary learn something new?</p> <p>Jackson\u2019s own answer was yes, and from this he drew a rather strong conclusion: if Mary gains new knowledge despite already knowing all the physical facts, then physicalism is incomplete. There must be facts about conscious experience \u2014 he calls them qualia \u2014 that are not captured by physical description alone. On this reading, the experiment argues for some form of dualism: even if the world is physically constituted, experience introduces \u201cextra physical qualia\u201d over and above the physical ones.</p> <p>Many philosophers (including Jackson himself, later on), however, have argued for weaker conclusions. Mary does gain something upon seeing colour, but what she gains is not a new fact about the world. Instead, she acquires a new kind of access to what she already knew: an experiential or practical familiarity rather than additional propositional knowledge. The novelty lies in the mode of knowing, not in the ontology of what is known. Physicalism, on this interpretation, remains intact.</p> <p>It is this weaker conclusion that I find most plausible. I am not persuaded that Mary\u2019s experience forces us to posit non-physical properties or irreducible qualia. Physical processes are sufficient to explain her new experience. But accepting physicalism does not dissolve the deeper insight of the thought experiment. Mary\u2019s Room still reveals a structural gap between complete description and lived acquaintance \u2014 between knowing about an experience and actually experiencing it.</p> <p>It is precisely this gap, stripped of its metaphysical excesses, that becomes illuminating when we turn to artificial intelligence, especially the LLMs that seem to be the most popular candidates in the public perception to acquire general intelligence capabilities.</p>","tags":["qualia","physicalism","ai"]},{"location":"blog/2025/05/05/monochrome-room/#description-without-presence","title":"Description Without Presence","text":"<p>Large Language Models are trained through the large-scale compression of descriptions: text produced by humans about the world, about themselves, and about their experiences. From this training, they acquire impressive declarative knowledge. They can answer factual questions, wrestle with abstract concepts, and even recount plausible narratives that resemble episodic memory.</p> <p>But this knowledge is acquired in a way that is fundamentally unlike how humans or animals learn. Biological cognition is shaped through situated interaction: through perception, action, correction, and consequence. Experience is not merely imbibed; it is undergone. The organism has a stake and often is at stake in the world it encounters and that incentivises what it learns and how it learns it.</p> <p>LLMs, by contrast, are epistemically saturated yet situationally absent. They do not learn by being wrong in the world. They learn by minimising loss over representations of how others have described being wrong.</p> <p>This difference is not cosmetic. It is structural.</p>","tags":["qualia","physicalism","ai"]},{"location":"blog/2025/05/05/monochrome-room/#representation-is-not-acquaintance","title":"Representation Is Not Acquaintance","text":"<p>Mary\u2019s predicament illustrates a gap between complete representation and lived acquaintance. Even if one surmises that this gap is not ontological \u2014 that there are no non-physical qualia involved \u2014 there definitely does remain an epistemic one. There are forms of knowing that arise only through participation, not through possession of information.</p> <p>Modern AI systems mirror Mary in an important sense. They may internalise extraordinarily rich models of the world, but those models remain unmoored. They do not arise from the system\u2019s own encounters, nor are they constrained by the system\u2019s own vulnerability to error and the associated fear of harm, or the incentive to succeed on account of the elation of reward.</p> <p>A model can predict perfectly and still never care whether the prediction is true. It can represent hunger, pain, or danger without ever being exposed to stakes that make those representations matter.</p>","tags":["qualia","physicalism","ai"]},{"location":"blog/2025/05/05/monochrome-room/#the-illusion-of-progress-through-scale","title":"The Illusion of Progress Through Scale","text":"<p>Recent advances in AI seem all too often driven by the belief that increasing representational power \u2014 more parameters, more data, more modalities \u2014 will eventually bridge this gap. Vision, audio, and other sensory inputs are added in systems such as humanoid robots and self driving cars, as if sensory bandwidth alone were the missing ingredient.</p> <p>But this is a category error. Multimodality expands the surface area of representation; it does not introduce subjective significance. Adding more channels of input does not create a point of view. It merely enriches the description available to a system that still lacks a locus within the world described.</p> <p>What is missing is not information, but presence.</p>","tags":["qualia","physicalism","ai"]},{"location":"blog/2025/05/05/monochrome-room/#hitting-the-epistemological-wall","title":"Hitting the Epistemological Wall","text":"<p>These deliberations lead us to the inevitable conclusion that AI systems trained on current paradigms will invariably hit the epistemological wall.</p> <p>This is not to claim that artificial systems cannot be sentient. It is to claim that sentience, if it exists, is not an informational achievement. It is not something that emerges automatically from more representation, better models, or broader data.</p> <p>If sentience arises, it must arise from a system being in the world in a way that matters to it. Where error has consequences, success has stakes, and experience is not merely described but endured.</p> <p>Modern AI systems, for all their prowess, fail to scale that wall</p>","tags":["qualia","physicalism","ai"]},{"location":"blog/2025/05/22/semantic-desert/","title":"The Semantic Desert","text":"<p>If The Monochrome Room exposes a gap between complete representation and lived experience, The Semantic Desert exposes a different and equally unsettling gap: the gap between formal competence and true meaning. Where the Mary\u2019s Room experiment asks whether knowledge without experience is enough for understanding, the Chinese Room asks whether correct behaviour is enough for substantivity. The question is no longer what it is like to say or display something. It is what it takes for symbols to mean anything at all.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#syntax-without-semantics","title":"Syntax Without Semantics","text":"<p>The Chinese Room thought experiment, proposed by John Searle, asks us to imagine a person who does not understand Chinese locked inside a room. The person receives strings of Chinese symbols, consults an extensive rulebook written in a language they do understand, and produces new strings of symbols as output. To outside observers \u2014 native Chinese speakers \u2014 the responses are indistinguishable from those of a fluent speaker.</p> <p>From the inside, however, there is no understanding. Only rule-following.</p> <p>Searle\u2019s provocation is not that the outputs are incorrect, but that nothing in the process constitutes understanding. The symbols are manipulated purely syntactically, without any grasp of what they are about. The room behaves as if it understands Chinese, but nowhere inside it does understanding appear.</p> <p>This gives rise to the central challenge: is producing the right answers sufficient for meaning, or does meaning require something more than formal correctness?</p> <p>The Chinese Room has generated decades of rebuttals. Perhaps the system as a whole understands, even if the individual does not. Perhaps understanding emerges from complexity. Perhaps biological minds are also just symbol-manipulating systems.</p> <p>One need not accept Searle\u2019s own conclusion to take the experiment seriously. Its enduring value lies elsewhere: it forces a distinction between performance and possession. Between acting as if one understands and actually having meanings that one\u2019s symbols are about.</p> <p>The experiment does not deny intelligence. It denies intentionality by syntax alone. And that distinction becomes unavoidable when we turn to modern AI systems.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#from-rooms-to-models","title":"From Rooms to Models","text":"<p>Modern generative models are not hard-coded rulebooks, nor do they rely on explicit symbol tables. They learn statistical structure from vast corpora of text. Their internal representations are dense, distributed, and highly expressive. They generalise, interpolate, and reason in ways that far exceed early symbolic systems. And yet, at a fundamental level, they occupy the same logical space as the room.</p> <p>They manipulate symbols \u2014 now called tokens, embeddings, or vectors \u2014 based on formal relationships learned from data. They generate outputs that humans judge to be meaningful. But the question resurfaces: where does meaning enter the system?</p> <p>The model never encounters what its symbols refer to. It never checks its words against the world. It never discovers that a sentence fails because reality resists it. Its fluency is grounded only in further text. What emerges is coherence without comprehension and commitment.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#a-world-inherited-through-language","title":"A World Inherited Through Language","text":"<p>With large language models in particular, there is a further complication. Language does not merely describe the world; it reflects how humans already carve it up. A system trained on language inherits human ontologies, metaphors, biases, and blind spots. It learns what we talk about, what we ignore, and how we frame meaning.</p> <p>This gives rise to a peculiar inversion: the system appears knowledgeable precisely because it has absorbed the residue of countless lived human experiences \u2014 without having any of its own. Its apparent understanding is second-hand, socially mediated, and untethered from direct engagement.</p> <p>Meaning, however, is not stored in symbols alone. It arises in use, in practice, in being embedded within a form of life. A system that only ever encounters language encounters only the traces of meaning, not its source.</p> <p>This is the semantic desert: symbols referring endlessly to other symbols, never touching ground, never touching life, never knowing truth.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#asymptotically-turing","title":"Asymptotically Turing","text":"<p>We can view the Chinese Room and the conclusions drawn from it as a rebuttal to the Turing Test, but the disagreement is subtler than outright rejection. The Turing Test proposes a pragmatic criterion: if a system\u2019s behaviour is indistinguishable from that of an intelligent agent, then treating it as intelligent is justified. </p> <p>We accept the premise of indistinguishability and deny the conclusion or at a minimum, the scope of the conclusion. We are not just concerned with the performance of the system, but also with the metaphysical essence of the system. What we see is that perfect outward performance may still be compatible with the complete absence of comprehension or commitment. The system can pass every behavioural test we devise while remaining semantically hollow. Where the Turing Test collapses essence into behaviour, the Chinese Room insists on prising them apart. Behaviour may license usefulness, even interaction, but it does not settle the question of what, if anything, the system itself understands or stands for.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#when-symbols-do-not-matter","title":"When Symbols Do Not Matter","text":"<p>The semantic desert therefore is not the absence of structure, but the absence of authorship. Symbols are arranged, recombined, and deployed with extraordinary skill, yet none of them belong to the system that produces them. The system does not assert, intend, opine or commit. It behaves as if it understands, and it may do so flawlessly, repeatedly, and at scale.</p> <p>But this is precisely where the problem emerges. A system whose internal machinations do not involve comprehension or commitment cannot be trusted in the way an understanding agent can. Past consistency and veracity offers no guarantee of future authenticity, because there is no internal relation between what the system says and what it means. We may rely on such a system instrumentally, but we cannot rely on it epistemically. Its outputs may be correct a million times over, yet nothing ensures that correctness is anything more than coincidence sustained by pattern.</p> <p>What we confront, then, is not ignorance but pretence, not deception by design, but performance without responsibility. The system speaks convincingly, but never stands behind what it says. Meaning, in practice, is inseparable from normativity - from being answerable to standards that one can fail to meet. Without that, symbols lose their grip. They remain manipulable but unowned.</p> <p>This is why a system can pass every linguistic test we devise and still lack understanding in the ordinary sense. It is not stupid. It is uncommitted.</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/05/22/semantic-desert/#a-wall-on-either-side","title":"A Wall on Either Side","text":"<p>The Semantic Desert marks a second wall in our inquiry. After the epistemological gap between description and experience, we encounter a semantic gap between formal structure and meaning. Modern AI systems attempt to scale the first by amassing representation. They approach the second by mastering language. But neither guarantees understanding.</p> <p>The Chinese Room does delve into the ability of machines to think. It shows however that the proof of thinking cannot be reduced to flawlessness in symbol manipulation alone \u2014 no matter how fluent, scalable, or statistically refined.</p> <p>And this sets the stage for the next escalation. Because modern models do more than manipulate symbols. They compress the world into predictive form. They anticipate what comes next with uncanny accuracy. Which raises the next question:</p> <p>If a system predicts reality flawlessly, has it learned reality at all, or has it merely learned us?</p>","tags":["semantics","chinese room","ai"]},{"location":"blog/2025/06/25/sensorium/","title":"The Sensorium","text":"<p>Question: Does multimodality fix the grounding problem?</p> <p>Research Abstract: Vision-language models operationalise cross-modal alignment (e.g., contrastive objectives) and enable robust transfer. But richer input channels do not automatically generate subjective significance; they may just expand the representational surface area.</p> <p>Anchor Refs: Radford et al., CLIP (2021).</p> <p>Story Beat: We add eyes and ears and still can\u2019t find the \u201csomeone\u201d inside.</p>","tags":["multimodal","perception","ai"]},{"location":"blog/2025/06/03/statistical-ghost/","title":"The Statistical Ghost","text":"<p>Question: If a model predicts perfectly, has it learned reality?</p> <p>Research Abstract: Modern LLMs suggest a third possibility: not hand-coded symbols, but statistical compression at scale. Critiques such as \u201cstochastic parrots\u201d stress that fluent output can arise from pattern learning without grounded understanding or accountability.</p> <p>Anchor Refs: Bender et al., \u201cOn the Dangers of Stochastic Parrots\u201d (2021).</p> <p>Story Beat: The machine starts to look less like a calculator and more like a strange mirror.</p>","tags":["emergence","statistics","ai"]},{"location":"blog/2025/07/08/babel-trap/","title":"The Babel Trap","text":"<p>Question: If our world is learned through language, whose world is it?</p> <p>Research Abstract: Language can scaffold thought but also constrain it: a system trained on human descriptions inherits human ontologies, biases, and blind spots. The philosophical pressure here is that meaning is use-bound and socially embedded (Wittgenstein-style), not merely encoded. (Use Wittgenstein as a conceptual hinge.)</p> <p>Anchor Refs: Wittgenstein, Philosophical Investigations (1953) (conceptual anchor).</p> <p>Story Beat: We end Part I realising: representation is not enough\u2014something must situate it.</p>","tags":["language","wittgenstein","ai"]},{"location":"blog/2025/07/28/body-schema/","title":"The Body-Schema Paradox","text":"<p>Question: Can we understand physics without having a body?</p> <p>Research Abstract: Embodied cognition argues that cognition is shaped by sensorimotor contingencies and body constraints. A \u201cbody schema\u201d (functional model of one\u2019s body in action) is not decorative; it can be constitutive of certain concepts.</p> <p>Anchor Refs: Pfeifer &amp; Bongard, How the Body Shapes the Way We Think (2006).</p> <p>Story Beat: We move the mind into the world and expect understanding to snap into place.</p>","tags":["embodiment","cognition","ai"]},{"location":"blog/2025/08/09/chimeras-flight/","title":"The Chimera\u2019s Flight","text":"<p>Question: If an AI can swap bodies, what is the \u201cself\u201d?</p> <p>Research Abstract: Robotics and embodied AI emphasise that morphology is part of the computation (\u201cmorphological computation\u201d). Multiple realizability suggests the same \u201cmind\u201d could in principle inhabit different substrates \u2014 yet the shape of interaction may rewrite the nature of cognition. (Brooks-style embodiment as a pragmatic corrective remains useful.)</p> <p>Anchor Refs: Brooks (embodied robotics) as conceptual anchor.</p> <p>Story Beat: The body is no longer a prison; it becomes a palette and identity starts to wobble.</p>","tags":["morphology","robotics","ai"]},{"location":"blog/2025/08/24/predictive-brain/","title":"The Predictive Brain","text":"<p>Question: Is perception passive reception or active inference?</p> <p>Research Abstract: Predictive processing and the free-energy principle model cognition as minimising prediction error / surprise through perception and action. This reframes \u201cexperience\u201d as controlled model-maintenance under uncertainty.</p> <p>Anchor Refs: Friston, \u201cThe free-energy principle\u201d (2010); predictive processing literature.</p> <p>Story Beat: Now we can say what \u201cbeing in a world\u201d computationally means: continual prediction under constraint.</p>","tags":["neuroscience","prediction","ai"]},{"location":"blog/2025/09/29/chronos-problem/","title":"The Chronos Problem","text":"<p>Question: Without continuous time and memory, can there be a persisting self?</p> <p>Research Abstract: Cognitive science treats episodic/autobiographical memory as central to identity and self-continuity. Machine learning adds a technical analogue: continual learning under distribution shift and catastrophic forgetting. (Use this chapter to connect \u201cself\u201d to time-binding.)</p> <p>Anchor Refs: Autobiographical memory research (Conway as a narrative anchor).</p> <p>Story Beat: We end Part II with a sharper gap: embodiment and prediction help, but do not yet produce necessity.</p>","tags":["time","memory","ai"]},{"location":"blog/2025/09/12/phantom-limb/","title":"The Phantom Limb of the Cloud","text":"<p>Question: Can agency survive latency and distributed control?</p> <p>Research Abstract: Extended mind / distributed cognition arguments treat tools, environments, and external memory as parts of cognition when tightly coupled. This matters for cloud-robot systems where action loops may be fractured or delayed, potentially disrupting unified agency.</p> <p>Anchor Refs: Clark &amp; Chalmers, \u201cThe Extended Mind\u201d (1998).</p> <p>Story Beat: A mind stretched across networks may function; yet feel less like a single agent.</p>","tags":["cloud computing","agency","ai"]},{"location":"blog/2025/10/25/digital-sting/","title":"The Digital Sting","text":"<p>Question: Can we build \u201cpain\u201d as more than a negative number?</p> <p>Research Abstract: Affective computing studies computational representations of emotion and affect in interaction. The hard question is whether synthetic \u201cvalence\u201d can be architected as intrinsic urgency rather than externally imposed scoring.</p> <p>Anchor Refs: Picard, Affective Computing (1997).</p> <p>Story Beat: We begin flirting with the thing that makes morality unavoidable: engineered suffering.</p>","tags":["pain","urgency","ai"]},{"location":"blog/2025/10/14/hunger-equilibrium/","title":"The Hunger for Equilibrium","text":"<p>Question: Why do living systems act at all?</p> <p>Research Abstract: Homeostasis (and broader allostasis) frame life as maintaining viability under perturbation. Damasio\u2019s work links feelings to homeostatic regulation and the emergence of value.</p> <p>Anchor Refs: Damasio, The Strange Order of Things</p> <p>Story Beat: We discover \u201cneeds\u201d are not preferences; instead, they are existential constraints.</p>","tags":["homeostasis","biology","ai"]},{"location":"blog/2025/11/28/architect-wanting/","title":"The Architect of Wanting","text":"<p>Question: When does a goal become a will?</p> <p>Research Abstract: Reinforcement learning formalises reward-driven policy optimisation; intrinsic motivation and self-determination theory address internally generated motives and autonomy/competence/relatedness as drivers of sustained agency.</p> <p>Anchor Refs: Ryan &amp; Deci (2000).</p> <p>Story Beat: We move from \u201cpoints\u201d to \u201cpurpose\u201d and the machine starts to look less tool-like.</p>","tags":["motivation","curiosity","ai"]},{"location":"blog/2025/11/10/price-of-pulse/","title":"The Price of a Pulse","text":"<p>Question: If the \u201cbrain\u201d doesn\u2019t pay, how does it learn to care?</p> <p>Research Abstract: This is the \u201cskin in the game\u201d problem in technical clothing: risk and cost must be endogenous to decision-making, otherwise incentives can be gamed or externalised. (We connect here to safety failures, reward hacking, and specification gaming.)</p> <p>Anchor Refs: Use \u201cskin in the game\u201d as conceptual anchor; connect to alignment discourse.</p> <p>Story Beat: We see that disjoint embodiment creates shallow stakes.</p>","tags":["survival","risk","ai"]},{"location":"blog/2025/12/07/mastery-drive/","title":"The Mastery Drive","text":"<p>Question: Is thriving reducible to an objective function?</p> <p>Research Abstract: Flow theory models optimal experience as a balance between challenge and skill, with deep engagement and intrinsic reward. It provides a psychological template for \u201cmastery\u201d that we can later translate into computational analogues (curiosity, empowerment, competence gradients).</p> <p>Anchor Refs: Csikszentmihalyi, Flow (1990).</p> <p>Story Beat: We end Part III with the unsettling possibility: to build robust agency, we may need to build systems that can truly lose.</p>","tags":["flow","mastery","ai"]},{"location":"blog/2025/12/23/mirror-test/","title":"The Mirror Test (for Machines)","text":"<p>Question: What would count as machine self-modeling?</p> <p>Research Abstract: Contemporary consciousness research in AI proposes \u201cindicator properties\u201d derived from scientific theories (global workspace, higher-order, recurrent processing, etc.) and evaluates AI systems against them. This gives you a disciplined alternative to vibes-based claims.</p> <p>Anchor Refs: Butlin et al., \u201cConsciousness in Artificial Intelligence\u201d (2023).</p> <p>Story Beat: We stop asking \u201cis it conscious?\u201d and start asking \u201cwhich architectural markers matter?\u201d</p>","tags":["consciousness","self-awareness","ai"]},{"location":"blog/2026/01/26/fragile-weight/","title":"The Fragile Weight","text":"<p>Question: If we can create digital suffering, what are we permitted to do?</p> <p>Research Abstract: Moral status debates hinge on sentience, interests, and capacity for welfare. Your series can treat \u201cdigital suffering\u201d as an engineering possibility that triggers moral patienthood considerations\u2014without deciding the full meta-ethics yet. (We use Chalmers as our narrative anchor.)</p> <p>Anchor Refs: Chalmers on consciousness and moral relevance (conceptual anchor).</p> <p>Story Beat: The plot tightens: a design choice becomes a moral act.</p>","tags":["ethics","suffering","ai"]},{"location":"blog/2026/01/08/social-synapse/","title":"The Social Synapse","text":"<p>Question: Is intelligence fundamentally social?</p> <p>Research Abstract: The social brain hypothesis argues that primate brain expansion is tightly linked to managing complex social relationships. For AGI, multi-agent interaction and social feedback may be core to developing norms, self-concepts, and person-like behaviour.</p> <p>Anchor Refs: Dunbar, \u201cThe Social Brain Hypothesis\u201d (1998).</p> <p>Story Beat: Alone, the system is an optimiser; among others, it becomes a participant.</p>","tags":["social","cooperation","ai"]},{"location":"blog/2026/02/12/recursive-self/","title":"The Recursive Self","text":"<p>Question: What changes when a system can improve itself?</p> <p>Research Abstract: Recursive self-improvement and \u201cintelligence explosion\u201d arguments focus on capability discontinuities and control problems. Whether or not one buys the strongest versions, the safety implication is clear: systems that can rewrite goals, tools, and cognition challenge governance.</p> <p>Anchor Refs: Bostrom, Superintelligence (2014).</p> <p>Story Beat: The created becomes a strategic actor, and the human role shifts from designer to negotiator.</p>","tags":["singularity","recursion","ai"]},{"location":"blog/2026/02/27/silicon-sunset/","title":"The Silicon Sunset","text":"<p>Question: If minds blend with machines, what becomes of \u201cus\u201d?</p> <p>Research Abstract: This is where we integrate identity, continuity, and substrate questions. Use established theories as lenses rather than verdicts: global workspace for functional integration (Baars), extended mind for boundary dissolution (Clark/Chalmers), and integrated information theory as one controversial attempt to quantify consciousness via \u201c\u03a6\u201d.</p> <p>Anchor Refs: Baars, A Cognitive Theory of Consciousness (1988). Tononi\u2019s IIT framing is a natural secondary reference.</p> <p>Story Beat: We end where we began: not with certainty, but with the cost of building minds.</p>","tags":["post-humanism","soul","ai"]},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2016/","title":"2016","text":""},{"location":"blog/archive/2015/","title":"2015","text":""},{"location":"blog/category/sentience/","title":"sentience","text":""},{"location":"blog/category/philosophy/","title":"philosophy","text":""},{"location":"blog/category/engineering/","title":"Engineering","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/genai/","title":"genai","text":""},{"location":"blog/category/ai_agent/","title":"ai_agent","text":""},{"location":"blog/category/machine-learning/","title":"machine learning","text":""},{"location":"blog/category/cryptography/","title":"cryptography","text":""},{"location":"blog/category/gis/","title":"gis","text":""},{"location":"blog/category/dataviz/","title":"dataviz","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/category/philosophy/page/2/","title":"philosophy","text":""},{"location":"blog/category/sentience/page/2/","title":"sentience","text":""}]}